{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <h1> \n",
    "        Data Camp Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <center>\n",
    "    <h1> \n",
    "        Scrapping Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "    <em>\n",
    "        See 'scrapping/scrapping_functions.py' file for the designed functions\n",
    "    </em>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installing & Importing dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Packages required \n",
    "* requests\n",
    "* html5lib\n",
    "* bs4\n",
    "* bs4 selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from parse import search\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to the chromedriver\n",
    "driver_path = r\"chromedriver.exe\"\n",
    "\n",
    "#Instagram account infos\n",
    "user_name = \"datacamp1020\"\n",
    "password= \"$datacamp1020$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all this notebook we will use three functions of a class designed for this challenge. They are available and called from the file 'scraping_functions.py' (in the data folder of the repo, the same as this notebook is in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designed scrapping functions see 'scrapping_functions.py' \n",
    "from scrapping_functions import Scrapper\n",
    "# Define our Scrapper class\n",
    "scrapper = Scrapper(user_name, password, driver_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retrieving top 49 World Influencers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a designed scrapping function to connect to this [Wikipedia web page](https://fr.m.wikipedia.org/wiki/Liste_des_comptes_Instagram_les_plus_suivis) to retrieve the 50 most followed instagram accounts (brands', institutions' or magazines' accounts will be deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "influencers_int, links_int = scrapper.get_influencers_world()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = influencers_int.to_csv (r'0_scrapping_data\\influencer_list_world.csv', index = None, header=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieving influencers infos and their posts' links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again use two designed functions (avalaible in the scrapping_functions.py file) to connect to instagram using the account informations. The function goes to each followers instagram page and scroll to retrieve a list of publications for each one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can select only a few numbers of influencers to retrieve posts from to prevent connection problems or account ban \n",
    "#links_int_1 = links_int[0:10]\n",
    "#influencers_int_1 = influencers_int[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames=[]\n",
    "pause_time = 3\n",
    "for i in range(len(links_int)):\n",
    "    print(links_int[i])\n",
    "    if i == 0:\n",
    "        browser = scrapper.connection_instagram()\n",
    "    df = scrapper.get_influencer_posts(influencers_int,i, browser, links_int[i], pause_time)\n",
    "    data_frames.append(df)\n",
    "links_pub_int = pd.concat(data_frames)\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_pub_int = pd.concat(data_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = links_pub_int.to_csv(r'0_scrapping_data\\publications_list.csv', index = None, header=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieving posts' infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the links scrapped right above to go to each one and scrap the related informations (use of the designed function get_info_pub of the class Scrapper in the file scrapping_functions.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications = pd.read_csv(r'0_scrapping_data\\publications_list.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_post = [] \n",
    "browser = scrapper.connection_instagram()\n",
    "for pub_link in publications[\"Links to publication\"]: \n",
    "    print(publications[publications[\"Links to publication\"]==pub_link].index[0])\n",
    "    df = scrapper.get_pub_info(pub_link,browser)\n",
    "    df_post.append(df)\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_info = pd.concat(df_post, ignore_index = True)\n",
    "posts_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.merge(publications, post_info, how ='right', on =\"Links to publication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = df_2.to_csv (r'0_scrapping_data\\raw_final_dataset.csv', index = None, header=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the merged dataset ollected from scrapping right above and process to a simple data pre-processing for convenience and prevent polluting the main notebook presenting the challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(r'0_scrapping_data\\raw_final_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting unecessary columns and renaming useful ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inst = raw_data.drop(columns = ['Links to publication', 'Links to profile', 'Account description',\n",
    "                       'Verified Status'])\n",
    "data_inst.columns = ['influencer','pr_activity','num_posts','num_followers','num_followings','post_description',\n",
    "             'num_comments','num_likes','posting_date','media_type']\n",
    "\n",
    "data_inst = data_inst.reset_index(drop=True)\n",
    "print(data_inst.shape)\n",
    "data_inst.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(row):\n",
    "    return str(row).replace(\",\",\"\").replace(\" \",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inst['num_likes'] = data_inst['num_likes'].apply(clean).astype(int)  \n",
    "data_inst['num_comments'] = data_inst['num_comments'].astype(int)\n",
    "data_inst['num_followers'] = data_inst['num_followers'].apply(clean).astype(int)  \n",
    "data_inst['num_followings'] = data_inst['num_followings'].apply(clean).astype(int) \n",
    "data_inst['num_posts'] = data_inst['num_posts'].apply(lambda x : x.replace(\"publications\",\"\"))\n",
    "data_inst['num_posts'] = data_inst['num_posts'].apply(lambda x : x.replace(\"posts\",\"\"))\n",
    "data_inst['num_posts'] = data_inst['num_posts'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-organizing professional activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inst['pr_activity'] = data_inst['pr_activity'].apply(lambda x : x.lower())\n",
    "data_inst['pr_activity'] = data_inst['pr_activity'].apply(lambda x : x.replace(' et ',''))\n",
    "data_inst['pr_activity'] = data_inst['pr_activity'].apply(lambda x : x.replace('cienne','cien'))\n",
    "data_inst['pr_activity'] = data_inst['pr_activity'].apply(lambda x : x.replace('trice','teur'))\n",
    "data_inst['pr_activity'] = data_inst['pr_activity'].apply(lambda x : x.replace('com√©dienne','acteur'))\n",
    "data_inst['pr_activity'] = data_inst['pr_activity'].replace({'footballeur':'Athlete',\n",
    "                                        'joueur de basket-ball':'Athlete','acteur':'Actor',\n",
    "                                        'acteurcatcheur professionnel':'Actor''musicienacteur':'Musician',\n",
    "                                        'joueur de cricket':'Athlete','ex-footballeur':'Athlete',\n",
    "                                        'acteurmusicien':'Actor','mod√®le':'Model','musicien':'Musician',\n",
    "                                         'personnalit√© de t√©l√©r√©alit√©mod√®le': 'TV Star',\n",
    "                                         'personnalit√© de t√©l√©r√©alit√©':'TV Star'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inst['pr_activity'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting brands and institutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inst = data_inst[data_inst['pr_activity']!='Marque']\n",
    "data_inst = data_inst[data_inst['pr_activity']!='r√©seau social']\n",
    "data_inst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inst['influencer'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timing features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inst['posting_date'] = pd.to_datetime(data_inst[\"posting_date\"])\n",
    "data_inst['year'] = data_inst['posting_date'].dt.year\n",
    "data_inst['month'] = data_inst['posting_date'].dt.month\n",
    "data_inst['day'] = data_inst['posting_date'].dt.day\n",
    "data_inst['hour'] = data_inst['posting_date'].dt.hour\n",
    "data_inst['Day_week'] = data_inst['posting_date'].dt.day_name()\n",
    "data_inst = data_inst.drop(columns = ['posting_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_inst.shape)\n",
    "data_inst.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = data_inst.to_csv (r'0_scrapping_data\\data_pre_processed.csv', index = None, header=True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
