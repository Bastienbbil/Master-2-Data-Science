{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Implémentation MuZero - Projet Reinforcement Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CR09s9tRyNG"
      },
      "source": [
        "<center> \r\n",
        "    <h1> \r\n",
        "        Projet Reinforcement Learning\r\n",
        "    <h2> \r\n",
        "        Notebook algorithme MuZero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdEUDpVCKNqT"
      },
      "source": [
        "<center>\r\n",
        "<img src=\"https://venturebeat.com/wp-content/uploads/2020/04/260dcb5d-b808-40f3-947f-c28e349c3da4-e1585799405647.png?resize=1198%2C600&strip=all\" width=\"200px\"> <br>\r\n",
        "<img src=\"https://www.telecom-sudparis.eu/wp-content/uploads/2020/02/81889345d20c3cbe29ad.png\" width=\"200px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR8EMagsJywU"
      },
      "source": [
        "<div style=\"text-align: right\">\r\n",
        "    <em>\r\n",
        "        Auteurs (article) : Julian Schrittwieser, Ioannis Antonoglou, et al. <br>\r\n",
        "        Auteur (code initial) : Johan Gras : https://github.com/johan-gras/MuZero <br>\r\n",
        "        Auteurs (rapport et notebook) : Billiot Bastien, Demay Ulysse, Ilbert Romain <br>\r\n",
        "        Professeur : E. Le Pennec \r\n",
        "        <br>\r\n",
        "        Mars 2021 \r\n",
        "    </em>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zkg1jMtZi6uQ"
      },
      "source": [
        "## Sommaire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCYuq6jXF6T8"
      },
      "source": [
        "* [Introduction à notre travail d'implémentation](#introduction) \r\n",
        "    * [Rappel des motivations des auteurs et de leur choix de jeux pour l'implémentation](#auteurs)\r\n",
        "    * [Notre travail d'implémentation](#implementation)\r\n",
        "    * [Structure](#structure)\r\n",
        "* [Pré-requis](#pre-requis)\r\n",
        "* [Notion de jeu](#jeu)\r\n",
        "    * [Classes abstraites](#classes_abstraites)\r\n",
        "    * [Standardisation des observations](#standardisation)\r\n",
        "    * [Le jeu du CartPole](#jeu_cartpole)\r\n",
        "    * [Autre jeu : Acrobot](#jeu_acrobot)\r\n",
        "* [Réseaux](#reseaux)\r\n",
        "    * [Les classes de base](#classes_base)\r\n",
        "    * [Sauvegarde du réseau](#sauvegarde_reseau)\r\n",
        "    * [Le réseau du CartPole](#reseau_cartpole)\r\n",
        "    * [Autre jeu : Acrobot](#reseau_acrobot)\r\n",
        "* [Configuration pour l'algorithme](#config)\r\n",
        "* [Monte Carlo Tree Search (MCTS)](#MCTS)\r\n",
        "    * [L'implémentation de la sélection](#selection)\r\n",
        "    * [L'implémentation de l'expansion d'un noeud](#expansion)\r\n",
        "    * [L'implémentation de la rétropropagation](#retropropagation)\r\n",
        "    * [Excécution de la recherche MCTS et sélection de l'action réalisée](#excecution)\r\n",
        "* [Entraînement](#entrainement)\r\n",
        "    * [Replay Buffer](#replay_buffer)\r\n",
        "    * [Self-play](#self-play)\r\n",
        "    * [Training](#training)\r\n",
        "* [MuZero](#muzero)\r\n",
        "* [Sauvegarde / Chargement d'un réseau entraîné](#reseau_entraine)\r\n",
        "* [Exécution de MuZero](#exec_muzero)\r\n",
        "    * [Exécution pour le CartPole](#exec_cartpole)\r\n",
        "        * [Entraînement from scratch](#from_scratch)\r\n",
        "        * [Exemple de sauvegarde et chargement du réseau obtenu](#ex_save)\r\n",
        "            * [Sauvegarde](#sauvegarde)\r\n",
        "            * [Chargement](#chargement)\r\n",
        "        * [Résultats pour plusieurs entraînement](#resultats)\r\n",
        "    * [Exécution pour Acrobot](#exec_acrobot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwLb2QFnDvjL"
      },
      "source": [
        "## Introduction à notre travail d'implémentation <a class=\"anchor\" id=\"introduction\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqSplBdLGImW"
      },
      "source": [
        "### Rappel des motivations des auteurs et de leur choix de jeux pour l'implémentation <a class=\"anchor\" id=\"auteurs\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjR4xKA91wl_"
      },
      "source": [
        "En plus de leur article, les auteurs de *Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model* fournissent un pseudo-code pour implémenter l'algorithme *MuZero*. Il existe plusieurs manières d'implémenter cet algorithme qui peuvent dépendre des jeux considérés. Dans le cadre de leur article, les auteurs proposent donc d'implémenter leur algorithme pour les jeux de Go, shogi, d'échecs et Atari. En effet, comme présenté dans notre rapport, la motivation de l'article et de *MuZero* est de construire un algorithme d'apprentissage *model-based* qui permet notamment d'obtenir des performances à l'état de l'art dans les problèmes classiquement réalisés par des algorithmes *model-based*. Ces résultats sont notamment obtenus par *AlphaZero* et les problèmes sont les jeux d'échecs, le jeu de Go et le Shogi. L'intérêt principal de MuZero est d'en plus de cela d'obtenir des performances à l'état de l'art dans des problèmes comme Atari 2600 qui sont traditionnellement obtenus par des méthodes *model-free*, donc non obtenus par exemple par *AlphaZero*. C'est pourquoi l'implémentation est centrée sur ces jeux-ci. Dans notre rapport, nous présentons notamment les résultats obtenus par les auteurs sur ces jeux et donc la réponse à la motivation de l'article.\r\n",
        "\r\n",
        "Cependant, nous avons décidé, plutôt que de refaire l'algorithme sur les mêmes jeux et obtenir des résultats similaires d'implémenter MuZero sur d'autres jeux. Ce choix permet notamment de montrer notre compréhension de l'algorithme mais aussi notre capacité à l'implémenter sur un nouveau jeu. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlzV2056ELxV"
      },
      "source": [
        "### Notre travail d'implémentation <a class=\"anchor\" id=\"implementation\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbhH8Z9CEN46"
      },
      "source": [
        "Pour ce projet, nous allons nous intéresser à l'implémentation de *MuZero* de Johan Gras, fidèle au pseudo-code et principalement adaptée pour les jeux un-joueur. Cette implémentation peut fonctionner pour différents jeux mais le code disponible sur le dépôt Git la particularise seulement pour le CartPole, jeu un joueur en 1D qui ne nécessite donc pas de Convolutional Neural Networks. Afin de pouvoir avoir une implémentation fonctionnelle de *MuZero* nous avons donc repris le code du Johan Gras dont le dépôt Git est le suivant : https://github.com/johan-gras/MuZero. Nous avons mis ce code dans un notebook afin de pouvoir l'expliquer au mieux et d'en dérouler les étapes de manière similaire à l'article.\r\n",
        "\r\n",
        "Il existe plusieurs implémentation de l'algorithme *MuZero*, la plus connue étant celle de Werner Duvaud disponible ici : https://github.com/werner-duvaud/muzero-general. Cette implémentation est très complète et propose de nombreux blocs supplémentaires comme la sauvegarde de réseau, un grand nombre de jeux prix en charge, une visualisation en console et en fenêtre des jeux mais aussi un monitoring de l'entraînement via Tensorboard. Nous n'avons pas choisi d'utiliser ce dépôt afin d'obtenir un code plus concis et plus fidèle à l'article mais aussi de pouvoir expliquer le code en détail en se concentrant sur l'algorithme *MuZero* lui-même et non les différents ajouts qui auraient pu être faits.\r\n",
        "\r\n",
        "Enfin, la valeur ajoutée de ce notebook par rapport au code présent sur le dépôt Git est son explication détaillée grâce à un découpage en parties correspondant aux concepts de l'article, de nombreux paragraphes entre les différentes cellules mais aussi des commentaires dans le code afin de mieux comprendre l'action de fonctions importantes. De plus, nous avons ajouté une section *Sauvegarde / Chargement d'un réseau entraîné* permettant de sauvegarder un réseau entraîné pour un jeu dans un dossier choisi. L'affichage de graphes pour visualiser l'entraînement de l'algorithme n'est pas non plus présent dans le dépôt de Johan Gras.\r\n",
        "\r\n",
        "Enfin, si les auteurs de notre article implémentent *MuZero* sur les jeux d'échecs, le jeu de Go, le Shogi et Atari 2600, l'implémentation de Johan Grasse concentre uniquement sur le jeu du CartPole. Afin d'aller plus loin nous proposons l'implémentation sur un autre jeu : Acrobot. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLK3e1gTK1mT"
      },
      "source": [
        "### Structure <a class=\"anchor\" id=\"structure\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eURPGmbflwVK"
      },
      "source": [
        "La structure de ce notebook est la suivante:\r\n",
        "- La section **Notion de jeu** contient l'implémentation d'un jeu vu comme son état à un tour donné. Elle permet notamment de calculer les policy, value et reward cibles utilisées pour l'entraînement.\r\n",
        "- La section **Réseaux** contient l'architecture du réseau de notre algorithme adapté pour le jeu choisi (ici CartPole et Acrobot) ainsi que tous les éléments entraînables.\r\n",
        "- La section **Configuration pour l'algorithme** donne une structure permettant de fournir à l'algorithme MuZero tous les hyperparamètres, la classe qu'elle contient est abstraite et doit être héritée et redéfinie pour le jeu choisi.\r\n",
        "- La section **Monte Carlo Tree Search (MCTS)** contient l'implémentation des arbres de recherche de Monte Carlo servant à choisir une action à effectuer à partir de l'état d'un jeu à un tour donnée.\r\n",
        "- La section **Entraînement** contient le `ReplayBuffer`, objet stockant toutes les parties simulées à partir du réseau tel qu'il est en début de boucle d'entraînement et dont les instants peuvent être échantillonnés pour l'entraînement. Elle contient aussi la définition de la loss et la mise à jour des poids grâce à l'optimiseur choisi et le `Self-Play` qui permet de simuler des parties du jeu choisi à partir du réseau tel qu'il est à une boucle d'entraînement donnée. On peut stocker ces parties dans le `ReplayBuffer` lors de la phase d'entraînement mais aussi évaluer notre modèle sur ces parties simulées en mode évaluation.\r\n",
        "- La section **MuZero** combine les différents éléments précédents pour pouvoir exécuter MuZero de son initialisation à l'entraînement de son réseau et son évaluation.\r\n",
        "- La section **Sauvegarde / Chargement d'un réseau entraîné** contient la fonction que nous avons ajouté à l'implémentation originale. Elle permet de sauvegarder un réseau entraîné dans un dossier choisi et de le recharger pour en poursuivre l'entraînement.\r\n",
        "- La section **Exécution de MuZero** est un exemple d'exécution de l'algorithme pour le jeu du CartPole dont la configuration est donnée ainsi qu'un graphe présentant les score de train et d'évaluation obtenus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMiauHTOVB_g"
      },
      "source": [
        "## Pré-requis <a class=\"anchor\" id=\"pre-requis\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9EVqVYEkrR-"
      },
      "source": [
        "import collections\r\n",
        "import math\r\n",
        "import typing\r\n",
        "from typing import Dict, List, Optional, Callable\r\n",
        "import random\r\n",
        "from itertools import zip_longest\r\n",
        "import pickle \r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.losses import MSE\r\n",
        "from tensorflow.keras import Model, regularizers\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.models import Sequential, load_model\r\n",
        "\r\n",
        "from abc import abstractmethod, ABC\r\n",
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUyqwkxyPCwg"
      },
      "source": [
        "MAXIMUM_FLOAT_VALUE = float('inf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5Il2BVGWEOW"
      },
      "source": [
        "La classe `Node` définie ci-dessous représente un noeud, nécessaire pour les arbres de recherche de Monte Carlo (MCTS). Nous devons la définir au début du notebook car elle est utile dans tout le code. En effet, à chaque tour MuZero s'appuie sur les MCTS et leurs différents noeuds qui correspondent aux états cachés du jeu.\r\n",
        "\r\n",
        "En plus de disposer des attributs classiques d'un noeud d'un arbre de recherche, ces noeuds possèdent un attribut `prior` afin d'incorporer le bruit d'exploration de Dirichlet dans la racine de l'arbre. Il est important de noter aussi qu'il stockent leur value et leur reward mises à jour lors du parcours de l'arbre."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRcaYbAdPVXB"
      },
      "source": [
        "class Node(object):\r\n",
        "    \"\"\"A class that represent nodes inside the MCTS tree\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, prior: float):\r\n",
        "        self.visit_count = 0 # Counts number of times the node is visited during the search \r\n",
        "        self.to_play = -1\r\n",
        "        self.prior = prior \r\n",
        "        self.value_sum = 0\r\n",
        "        self.children = {} # Contains the children node in the MCTS tree\r\n",
        "        self.hidden_state = None\r\n",
        "        self.reward = 0 # Immediate reward given by the dynamics function \r\n",
        "\r\n",
        "    def expanded(self) -> bool: # Is the node terminal or expanded (does it have chidren nodes or not ?)\r\n",
        "        return len(self.children) > 0 \r\n",
        "\r\n",
        "    def value(self) -> Optional[float]: \r\n",
        "        if self.visit_count == 0:\r\n",
        "            return None\r\n",
        "        return self.value_sum / self.visit_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIsC9Qj4-JfL"
      },
      "source": [
        "## Notion de jeu <a class=\"anchor\" id=\"jeu\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUZH3PyW-L2S"
      },
      "source": [
        "La première étape pour implémenter notre algorithme MuZero est d'implémenter la notion de jeu. En effet, les classes suivantes donnent un cadre pour les concepts de base nécessaire aux algorithmes de Reinforcement Learning : les actions (ainsi que l'historique des actions effectuées), les joueurs (non utile en pratique puisque l'algorithme joue seul) et surtout les jeux."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtJKm33EFbWS"
      },
      "source": [
        "### Classes abstraites <a class=\"anchor\" id=\"classes_abstraites\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK9ikFo5-Urx"
      },
      "source": [
        "class Action(object):\r\n",
        "    \"\"\" Class that represent an action of a game.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, index: int):\r\n",
        "        self.index = index\r\n",
        "\r\n",
        "    def __hash__(self):\r\n",
        "        return self.index\r\n",
        "\r\n",
        "    def __eq__(self, other):\r\n",
        "        return self.index == other.index\r\n",
        "\r\n",
        "    def __gt__(self, other):\r\n",
        "        return self.index > other.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTrm6qR8_Elq"
      },
      "source": [
        "class Player(object):\r\n",
        "    \"\"\"\r\n",
        "    A one player class.\r\n",
        "    This class is useless, it's here for legacy purpose and for potential adaptations for a two players MuZero.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __eq__(self, other):\r\n",
        "        return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWSxOgfl_ObD"
      },
      "source": [
        "class ActionHistory(object):\r\n",
        "    \"\"\"\r\n",
        "    Simple history container used inside the search.\r\n",
        "    Only used to keep track of the actions executed.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, history: List[Action], action_space_size: int):\r\n",
        "        self.history = list(history) #Initialize the actions' history container \r\n",
        "        self.action_space_size = action_space_size \r\n",
        "\r\n",
        "    def clone(self):\r\n",
        "        return ActionHistory(self.history, self.action_space_size)\r\n",
        "\r\n",
        "    def add_action(self, action: Action): # Creates the history through the action taken (abstract class Action defined two cells above)\r\n",
        "        self.history.append(action)\r\n",
        "\r\n",
        "    def last_action(self) -> Action: # Allows to retrieve last action taken\r\n",
        "        return self.history[-1]\r\n",
        "\r\n",
        "    def action_space(self) -> List[Action]: \r\n",
        "        return [Action(i) for i in range(self.action_space_size)]\r\n",
        "\r\n",
        "    def to_play(self) -> Player: # See abstract and unused class Player() in the above cell \r\n",
        "        return Player()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOdLfCaB_6SY"
      },
      "source": [
        "La classe suivante, `AbstractGame` est une classe abstraite implémentant la structure générale d'un jeu. Un jeu doit contenir les rewards et les actions associées à chaque tour mais aussi le facteur de discount pour calculer la value target. Il est important de noter qu'une instance de cette classe représente un seul état du jeu et est donc valide pendant un unique tour.\r\n",
        "\r\n",
        "Parmis les fonctions que cette classe implémente, la fonction `make_target()` est à noter. Celle-ci permet de calculer les targets, c'est à dire les policy, value et reward cibles pouvant être atteintes à l'issue d'un tour. Pour la value target il s'agit de la value à la racine multipliée par le facteur de discount exposant `td_steps` le nombre d'étapes hypothétiques futures choisies pour ce calcul, à laquelle s'ajoute la somme des rewards discountées entre cette derière étape hypothétique et la racine. C'est à partir de ces différentes targets (appelées quantités cibles dans le rapport) que l'algorithme pourra apprendre pendant la phase d'entraînement.\r\n",
        "\r\n",
        "Le jeu du CartPole implémenté par la suite hérite de cette classe et redéfinit les différentes fonctions abstraites propres à chaque jeu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6BGMTeo_Knx"
      },
      "source": [
        "class AbstractGame(ABC):\r\n",
        "    \"\"\"\r\n",
        "    Abstract class that allows to implement a game.\r\n",
        "    One instance represent a single episode of interaction with the environment.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, discount: float):\r\n",
        "        self.history = [] \r\n",
        "        self.rewards = []\r\n",
        "        self.child_visits = []\r\n",
        "        self.root_values = []\r\n",
        "        self.discount = discount\r\n",
        "\r\n",
        "    def apply(self, action: Action):\r\n",
        "        \"\"\"Apply an action onto the environment.\"\"\"\r\n",
        "\r\n",
        "        reward = self.step(action) # Apply the action on the environment \r\n",
        "        self.rewards.append(reward) # Stores the immediate reward of the action \r\n",
        "        self.history.append(action) # Stores the action in the ActionHistory\r\n",
        "\r\n",
        "    def store_search_statistics(self, root: Node):\r\n",
        "        \"\"\"After each MCTS run, store the statistics generated by the search.\"\"\"\r\n",
        "\r\n",
        "        sum_visits = sum(child.visit_count for child in root.children.values())\r\n",
        "        action_space = (Action(index) for index in range(self.action_space_size))\r\n",
        "        self.child_visits.append([\r\n",
        "            root.children[a].visit_count / sum_visits if a in root.children else 0\r\n",
        "            for a in action_space\r\n",
        "        ])\r\n",
        "        self.root_values.append(root.value())\r\n",
        "\r\n",
        "    def make_target(self, state_index: int, num_unroll_steps: int, td_steps: int, to_play: Player):\r\n",
        "        \"\"\"Generate targets to learn from during the network training.\"\"\"\r\n",
        "\r\n",
        "        # The value target is the discounted root value of the search tree td_steps steps\r\n",
        "        # into the future, plus the discounted sum of all rewards until then.\r\n",
        "        targets = []\r\n",
        "        for current_index in range(state_index, state_index + num_unroll_steps + 1):\r\n",
        "            bootstrap_index = current_index + td_steps\r\n",
        "            if bootstrap_index < len(self.root_values):\r\n",
        "                value = self.root_values[bootstrap_index] * self.discount ** td_steps # Computes the bootstrap index discounted target value term \r\n",
        "            else:\r\n",
        "                value = 0\r\n",
        "\r\n",
        "            for i, reward in enumerate(self.rewards[current_index:bootstrap_index]):\r\n",
        "                value += reward * self.discount ** i # Computes the target value for the training \r\n",
        "\r\n",
        "            if current_index < len(self.root_values):\r\n",
        "                targets.append((value, self.rewards[current_index], self.child_visits[current_index])) \r\n",
        "                # Stores the targets for the training : value which has just been caclulated \r\n",
        "            else:\r\n",
        "                # States past the end of games are treated as absorbing states.\r\n",
        "                targets.append((0, 0, []))\r\n",
        "\r\n",
        "        return targets\r\n",
        "\r\n",
        "    def to_play(self) -> Player:\r\n",
        "        \"\"\"Return the current player.\"\"\"\r\n",
        "        return Player()\r\n",
        "\r\n",
        "    def action_history(self) -> ActionHistory:\r\n",
        "        \"\"\"Return the actions executed inside the search.\"\"\"\r\n",
        "        return ActionHistory(self.history, self.action_space_size)\r\n",
        "\r\n",
        "    # Methods to be implemented by the child class\r\n",
        "    @property\r\n",
        "    @abstractmethod\r\n",
        "    def action_space_size(self) -> int:\r\n",
        "        \"\"\"Return the size of the action space.\"\"\"\r\n",
        "        pass\r\n",
        "\r\n",
        "    @abstractmethod\r\n",
        "    def step(self, action) -> int:\r\n",
        "        \"\"\"Execute one step of the game conditioned by the given action.\"\"\"\r\n",
        "        pass\r\n",
        "\r\n",
        "    @abstractmethod\r\n",
        "    def terminal(self) -> bool:\r\n",
        "        \"\"\"Is the game is finished?\"\"\"\r\n",
        "        pass\r\n",
        "\r\n",
        "    @abstractmethod\r\n",
        "    def legal_actions(self) -> List[Action]:\r\n",
        "        \"\"\"Return the legal actions available at this instant.\"\"\"\r\n",
        "        pass\r\n",
        "\r\n",
        "    @abstractmethod\r\n",
        "    def make_image(self, state_index: int):\r\n",
        "        \"\"\"Compute the state of the game.\"\"\"\r\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f0S4IRCBYZN"
      },
      "source": [
        "### Standardisation des observations <a class=\"anchor\" id=\"standardisation\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2Tor5oACXIe"
      },
      "source": [
        "The goal of this class is to take the observations and applying a min-max scaling on them so they are between -1 and 1.\r\n",
        "\r\n",
        "Cette classe permet de standardiser les observations via la méthode min-max, le but étant de les ramener entre -1 et 1 et que leur moyenne soit nulle. Pour ce faire, cette classe hérite de la classe `ObservationWrapper` déjà implémentée dans la bibliothèque `gym`. Les wrappers fournis par `gym` sont des outils puissants permettant d'ajouter des fonctionnalités aux différents environnements fournis, ici la standardisation des observations pour les rendre utilisables par notre algorithme.\r\n",
        "\r\n",
        "Les paramètres `low` et `high` passés à cette classe dépendent du jeu choisi, ce sont des vecteurs de la même dimension que les observations du jeu. Ils correspondent en fait aux valeurs maximales et minimales que peuvent prendre chacune des dimensions de l'observation. Pour les jeux de la bibliothèque `Gym` ces valeurs sont très souvent précisées dans le fichier implémentant le jeu dans le dossier https://github.com/openai/gym/tree/master/gym/envs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0d6SoPlBXen"
      },
      "source": [
        "class ScalingObservationWrapper(gym.ObservationWrapper):\r\n",
        "    \"\"\"\r\n",
        "    Wrapper that apply a min-max scaling of observations.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, env, low=None, high=None):\r\n",
        "        super().__init__(env)\r\n",
        "        assert isinstance(env.observation_space, gym.spaces.Box)\r\n",
        "\r\n",
        "        '''\r\n",
        "        Ces trois print permettent de voir la dimension des observations du jeu considéré\r\n",
        "        ainsi que leurs valeurs extrémales.\r\n",
        "        print(self.observation_space)\r\n",
        "        print(self.observation_space.low)\r\n",
        "        print(self.observation_space.high)\r\n",
        "        '''\r\n",
        "\r\n",
        "        low = np.array(self.observation_space.low if low is None else low)\r\n",
        "        high = np.array(self.observation_space.high if high is None else high)\r\n",
        "\r\n",
        "        self.mean = (high + low) / 2\r\n",
        "        self.max = high - self.mean\r\n",
        "\r\n",
        "    def observation(self, observation):\r\n",
        "        return (observation - self.mean) / self.max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hu2TfxCA4ik"
      },
      "source": [
        "### Le jeu du CartPole <a class=\"anchor\" id=\"jeu_cartpole\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW762Y9tBE8Z"
      },
      "source": [
        "Le jeu du CartPole consiste à essayer de maintenir droit un pendule inversé situé sur une plateforme pouvant se déplacer sur un axe horizontal en applicant des forces sur la plateforme. Pour l'implémenter la classe `CartPole` hérite de `AbstractGame` et s'appuie surtout sur l'environnement `CartPole-v1` présent dans la bibliothèque `Gym` (https://gym.openai.com/envs/CartPole-v1/) et dont le code source est disponible ici: https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py.\r\n",
        "\r\n",
        "Il est important de noter que l'attribut actions content la liste de toutes les actions possibles dans ce jeu, directement fournies par l'environnement. Les méthodes implémentées permettent notament d'effectuer une étape du jeu à partir de l'action choisie, d'indiquer si un jeu est terminé ou non ou encore de produire une observation à partir des états du jeu.\r\n",
        "\r\n",
        "Pour ce jeu les observations sont de dimension 4 et les dimensions du vecteur correspondent à `[position du cart, vitesse du cart, angle de la tige, vitesse de rotation de la tige]`. Il y a deux actions possibles qui sont déplacer le cart vers la droite ou le déplacer vers la gauche. La reward est de 1 pour chaque étape supplémentaire réussie (i.e. lorsque la barre ne tombe pas)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcjWjPB3A6-X"
      },
      "source": [
        "class CartPole(AbstractGame):\r\n",
        "    \"\"\"\r\n",
        "    The Gym CartPole environment\r\n",
        "    Define the abstract functions of the AbstractGame() class define previously to fit the CartPole game and environment\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, discount: float):\r\n",
        "        super().__init__(discount)\r\n",
        "        # Load the game environment from gym\r\n",
        "        self.env = gym.make('CartPole-v1')\r\n",
        "        # Use previously presented function to scale observations between -1 and 1 (null mean)\r\n",
        "        self.env = ScalingObservationWrapper(self.env, low=[-2.4, -2.0, -0.42, -3.5], high=[2.4, 2.0, 0.42, 3.5]) \r\n",
        "        # Initialize all possible actions of the game\r\n",
        "        self.actions = list(map(lambda i: Action(i), range(self.env.action_space.n)))\r\n",
        "        self.observations = [self.env.reset()]\r\n",
        "        self.done = False\r\n",
        "\r\n",
        "    @property\r\n",
        "    # Define useful functions specific to the game (concrete functions of class AbstractGame())\r\n",
        "    def action_space_size(self) -> int:\r\n",
        "        \"\"\"Return the size of the action space.\"\"\"\r\n",
        "        return len(self.actions)\r\n",
        "\r\n",
        "    def step(self, action) -> int:\r\n",
        "        \"\"\"Execute one step of the game conditioned by the given action.\"\"\"\r\n",
        "\r\n",
        "        observation, reward, done, _ = self.env.step(action.index)\r\n",
        "        self.observations += [observation]\r\n",
        "        self.done = done\r\n",
        "        return reward\r\n",
        "\r\n",
        "    def terminal(self) -> bool:\r\n",
        "        \"\"\"Is the game is finished?\"\"\"\r\n",
        "        return self.done\r\n",
        "\r\n",
        "    def legal_actions(self) -> List[Action]:\r\n",
        "        \"\"\"Return the legal actions available at this instant.\"\"\"\r\n",
        "        return self.actions\r\n",
        "\r\n",
        "    def make_image(self, state_index: int):\r\n",
        "        \"\"\"Compute the state of the game.\"\"\"\r\n",
        "        return self.observations[state_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3fN54sYvnhV"
      },
      "source": [
        "### Autre jeu : Acrobot <a class=\"anchor\" id=\"jeu_acrobot\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ1o49O9EI5v"
      },
      "source": [
        "En plus du jeu du CartPole implémenté nous avons tené d'ajouter un nouveau jeu, Acrobot. L'environnement de ce jeu est implémenté dans la bibliothèque `Gym` aussi et c'est un jeu en 1D. Les observations sont de dimension 6: `[cos(theta1) sin(theta1) cos(theta2) sin(theta2) thetaDot1 thetaDot2]`. Il y a 3 actions possibles: appliquer un couple de -1, 0 ou 1 sur la jointure entre les deux pendules. \r\n",
        "\r\n",
        "Le code source de ce jeu peut être trouvé sur la page https://github.com/openai/gym/blob/master/gym/envs/classic_control/acrobot.py. En parcourant un peu ce code on eut constater que la principale différence entre CartPole et ce jeu est que la reward peut être -1 ou 0 et donc elle n'est pas toujours positive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIh7AjYnv2b1"
      },
      "source": [
        "class Acrobot(AbstractGame):\r\n",
        "    \"\"\"The Gym Acrobot environment\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, discount: float):\r\n",
        "        super().__init__(discount)\r\n",
        "        self.env = gym.make('Acrobot-v1')\r\n",
        "        self.env = ScalingObservationWrapper(self.env, low=[-1, -1, -1, -1, -3.15, -3.15], high=[1, 1, 1, 1, 3.15, 3.15])\r\n",
        "        self.actions = list(map(lambda i: Action(i), range(self.env.action_space.n)))\r\n",
        "        self.observations = [self.env.reset()]\r\n",
        "        self.done = False\r\n",
        "\r\n",
        "    @property\r\n",
        "    def action_space_size(self) -> int:\r\n",
        "        \"\"\"Return the size of the action space.\"\"\"\r\n",
        "        return len(self.actions)\r\n",
        "\r\n",
        "    def step(self, action) -> int:\r\n",
        "        \"\"\"Execute one step of the game conditioned by the given action.\"\"\"\r\n",
        "\r\n",
        "        observation, reward, done, _ = self.env.step(action.index)\r\n",
        "        self.observations += [observation]\r\n",
        "        self.done = done\r\n",
        "        return reward\r\n",
        "\r\n",
        "    def terminal(self) -> bool:\r\n",
        "        \"\"\"Is the game is finished?\"\"\"\r\n",
        "        return self.done\r\n",
        "\r\n",
        "    def legal_actions(self) -> List[Action]:\r\n",
        "        \"\"\"Return the legal actions available at this instant.\"\"\"\r\n",
        "        return self.actions\r\n",
        "\r\n",
        "    def make_image(self, state_index: int):\r\n",
        "        \"\"\"Compute the state of the game.\"\"\"\r\n",
        "        return self.observations[state_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVFoLORADy_A"
      },
      "source": [
        "## Réseaux <a class=\"anchor\" id=\"reseaux\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTvXC8-zljqe"
      },
      "source": [
        "Dans cette section sont implémentés les réseaux que contient l'algorithme MuZero. En effet, ces derniers sont nécessaires pour définir :\r\n",
        "- La fonction de représentation, pour obtenir l'état interne initial au temps $t$ : $s^0$.\r\n",
        "- La fonction de dynamique, pour obtenir la reward immédiate $r^k$ et l'état interne $s^k$ à l'étape hypothétique $k$.\r\n",
        "- La fonction de prédiction, pour obtenir la politique $p^k$ et la value $v^k$ à l'étape hypothétique $k$.\r\n",
        "\r\n",
        "Plutôt que de faire un réseau par fonction, cette implémentation fait un réseau pour prédire la value, un pour la reward, un portant le nom de `dynamic_network` permettant de prédire l'état caché de l'étape hypothétique suivante, un dénommé `representation_network` servant de fonction de représentation et enfin un pour la politique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbVxDQl3Fo8L"
      },
      "source": [
        "### Les classes de base <a class=\"anchor\" id=\"classes_base\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBtwMW0lsgAS"
      },
      "source": [
        "Les 6 classes suivantes sont des classes nécessaires pour implémenter la structure de base du réseau de l'algorithme MuZero. La classe abstraite `BaseNetwork` qui représente cette structure devra ensuite être héritée par une classe représentant le réseau spécifique au jeu choisi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlMYf8EOuZbF"
      },
      "source": [
        "Cette classe implémente la sortie d'un réseau à la fin d'une étape hypothétique $k$, elle est constituée d'une value, une reward immédiate, une politique sous forme de liste de probabilités. Grâce à sa fonction `build_policy_logits` elle peut renvoyer la politique sous forme de dictionnaire associant à chaque action sa probabilité."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DLwSlA-FJVe"
      },
      "source": [
        "class NetworkOutput(typing.NamedTuple):\r\n",
        "    value: float\r\n",
        "    reward: float\r\n",
        "    policy_logits: Dict[Action, float]\r\n",
        "    hidden_state: typing.Optional[List[float]]\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def build_policy_logits(policy_logits):\r\n",
        "        return {Action(i): logit for i, logit in enumerate(policy_logits[0])}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8xCv4GjFLvA"
      },
      "source": [
        "class AbstractNetwork(ABC):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        self.training_steps = 0\r\n",
        "\r\n",
        "    @abstractmethod\r\n",
        "    def initial_inference(self, image) -> NetworkOutput:\r\n",
        "        pass\r\n",
        "\r\n",
        "    @abstractmethod\r\n",
        "    def recurrent_inference(self, hidden_state, action) -> NetworkOutput:\r\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chqKZq5S95qc"
      },
      "source": [
        "Le réseau uniforme est un réseau de base renvoyant dans tous les cas, peu importe que l'on soit en inférence initiale ou récursive, une value nulle, une reward nulle, un état interne vide et comme son nom l'indique une politique uniforme associant la même probabilité à chaque action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w9GU-jAFPCW"
      },
      "source": [
        "class UniformNetwork(AbstractNetwork):\r\n",
        "\r\n",
        "    def __init__(self, action_size: int):\r\n",
        "        super().__init__()\r\n",
        "        self.action_size = action_size\r\n",
        "\r\n",
        "    def initial_inference(self, image) -> NetworkOutput:\r\n",
        "        return NetworkOutput(0, 0, {Action(i): 1 / self.action_size for i in range(self.action_size)}, None)\r\n",
        "\r\n",
        "    def recurrent_inference(self, hidden_state, action) -> NetworkOutput:\r\n",
        "        return NetworkOutput(0, 0, {Action(i): 1 / self.action_size for i in range(self.action_size)}, None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq9UAYPp-J1k"
      },
      "source": [
        "Le modèle initial permet d'obtenir tous les éléments nécessaires à l'étape hypothétique 0 : l'état caché initial $s^0$ à l'instant $t$ à partir des observations du jeu à cet instant ainsi que la value initiale et la politique initiale. Il n'y a pas besoin de reward initiale puisque pour l'état initial celle-ci est nulle. \r\n",
        "\r\n",
        "Concernant l'état initial $k=0$, $s^{0}$ est obtenue par la fonction de représentation. La politique et la valeur sont ensuite obtenues grâce à la fonction de prediction (précisions données dans le rapport). La divergence entre le code est le papier est, comme précisé au dessus, qu'au lieu de faire un réseau pour chaque fonction (représentation, dynamique et prédiction), le code construit un réseau pour les différentes quantités. Plus précisément, il sépare la fonction dynamique en un réseau pour prédire la reward et un réseau pour prédire l'état caché de l'étape hypothétique suivante. La fonction de représentation est conservée en un réseau (permet de calculer $s^{0}$). Enfin, la fonction de prédiction est séparée en un réseau qui prédit la politique et un réseau qui prédit la value. \r\n",
        "\r\n",
        "Si l'on revient au modèle initial on va alors utiliser le réseau de la représentation pour obtenir $s^{0}$. Puis à partir de cette donnée, on utilise le réseau de la politique et le réseau de la value pour prédire ces quantités. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyHppCY7FSql"
      },
      "source": [
        "class InitialModel(Model):\r\n",
        "\r\n",
        "    def __init__(self, representation_network: Model, value_network: Model, policy_network: Model):\r\n",
        "        super(InitialModel, self).__init__()\r\n",
        "        self.representation_network = representation_network # Retrieve the representation function through the network\r\n",
        "        self.value_network = value_network # Retrieve value netork\r\n",
        "        self.policy_network = policy_network # Retrieve policy network\r\n",
        "\r\n",
        "    def call(self, image):\r\n",
        "        hidden_representation = self.representation_network(image) # s_0 value obtained via representation network (function)\r\n",
        "        value = self.value_network(hidden_representation) # Initial value from s_0 and the network of the value\r\n",
        "        policy_logits = self.policy_network(hidden_representation) # Initial value from s_0 and the network of the value\r\n",
        "        return hidden_representation, value, policy_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lZ-eD3H-lbb"
      },
      "source": [
        "Le modèle récurrent permet de passer à l'étape hypothétique suivante à partir de l'état caché et l'action de la précédente. Il est important de noter que celui-ci ne dispose pas directement de l'action mais plutôt de la variable `conditioned_hidden` constituée de la concaténation de l'état caché avec l'action. La production de cette variable est implémentée dans le réseau spécifique à notre jeu.\r\n",
        "\r\n",
        "Par rapport au modèle initial qui n'avait besoin que du réseau de représentation (pour calculer $s^{0}$), le réseau de prédiction de la politique et le réseau de prédiction de la value, les états cachés suivants n'utilisent plus le réseau de représentation. De plus, ils vont utiliser les réseaux qui représentent la fonction de dynamique : \r\n",
        "$$\r\n",
        "r^{k}, s^{k} = f_\\theta(s^{k-1}, a^{k})\r\n",
        "$$\r\n",
        "Le modèle possède pour ce faire, comme présenté plus tôt, le réseau pour prédire la reward $r^{k}$ à partir de `conditioned_hidden` (composé de $s^{k-1}$ et $a^{k}$) et le réseau pour prédire l'état caché $s^{k}$ de l'étape suivante hypothétique. Voyons ensuite comment cela s'organise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch5xFtjpFU9_"
      },
      "source": [
        "class RecurrentModel(Model):\r\n",
        "\r\n",
        "    def __init__(self, dynamic_network: Model, reward_network: Model, value_network: Model, policy_network: Model):\r\n",
        "        super(RecurrentModel, self).__init__()\r\n",
        "        self.dynamic_network = dynamic_network # Retrieve network to compute hypothetical step hidden state (from the previous hidden state and action in conditioned_hidden, \r\n",
        "                                               # first state initialized in above cell with representation_network)\r\n",
        "        self.reward_network = reward_network # Retrieve network to compute reward from hidden state and action (conditioned_hidden variable)\r\n",
        "        self.value_network = value_network # As in initial step, retrieve network to compute value from hidden state\r\n",
        "        self.policy_network = policy_network # As in initial step, retrieve network to compute policy from hidden state\r\n",
        "\r\n",
        "    def call(self, conditioned_hidden):\r\n",
        "        hidden_representation = self.dynamic_network(conditioned_hidden) # calculate s_k from s_k-1 and a_k (dynamic function quantity 1)\r\n",
        "        reward = self.reward_network(conditioned_hidden) # r_k from s_k-1 and a_k (dynamic function quantity 2)\r\n",
        "        value = self.value_network(hidden_representation) \r\n",
        "        policy_logits = self.policy_network(hidden_representation)\r\n",
        "        return hidden_representation, reward, value, policy_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rSJIs5w_ARX"
      },
      "source": [
        "La classe suivante est une classe abstraite représentant la structure de base du réseau de l'algorithme MuZero. Ce réseau permet de produire les éléments pour l'étape hypothétique initiale grâce à sa méthode `initial_inference` mais aussi de passer d'une étape hypothétique du jeu à la suivante grâce à la méthode `recurrent_inference`. La fonction `cb_get_variables` permet au réseau de renvoyer tous ses éléments pouvant être entraînés. Les autres fonctions sont abstraites et concernent plutôt la manière d'obtenir la value, la reward et la représententation jointe d'un état caché avec son action. En résumé, cette clase BaseNetwork construit la structure du jeu à partir des deux réseaux (initial et récurrent) pour pouvoir passer d'une étape à l'autre. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP9XC9l5ENj_"
      },
      "source": [
        "class BaseNetwork(AbstractNetwork):\r\n",
        "\r\n",
        "    def __init__(self, representation_network: Model, value_network: Model, policy_network: Model,\r\n",
        "                 dynamic_network: Model, reward_network: Model):\r\n",
        "        super().__init__()\r\n",
        "        # Networks blocks\r\n",
        "        self.representation_network = representation_network\r\n",
        "        self.value_network = value_network\r\n",
        "        self.policy_network = policy_network\r\n",
        "        self.dynamic_network = dynamic_network\r\n",
        "        self.reward_network = reward_network\r\n",
        "\r\n",
        "        # Models for inference and training\r\n",
        "        self.initial_model = InitialModel(self.representation_network, self.value_network, self.policy_network)\r\n",
        "        self.recurrent_model = RecurrentModel(self.dynamic_network, self.reward_network, self.value_network,\r\n",
        "                                              self.policy_network)\r\n",
        "\r\n",
        "    def initial_inference(self, image: np.array) -> NetworkOutput:\r\n",
        "        \"\"\"representation + prediction function\"\"\"\r\n",
        "\r\n",
        "        hidden_representation, value, policy_logits = self.initial_model.predict(np.expand_dims(image, 0)) \r\n",
        "        output = NetworkOutput(value=self._value_transform(value),\r\n",
        "                               reward=0.,\r\n",
        "                               policy_logits=NetworkOutput.build_policy_logits(policy_logits),\r\n",
        "                               hidden_state=hidden_representation[0])\r\n",
        "        return output\r\n",
        "\r\n",
        "    def recurrent_inference(self, hidden_state: np.array, action: Action) -> NetworkOutput:\r\n",
        "        \"\"\"dynamics + prediction function\"\"\"\r\n",
        "\r\n",
        "        conditioned_hidden = self._conditioned_hidden_state(hidden_state, action)\r\n",
        "        hidden_representation, reward, value, policy_logits = self.recurrent_model.predict(conditioned_hidden)\r\n",
        "        output = NetworkOutput(value=self._value_transform(value),\r\n",
        "                               reward=self._reward_transform(reward),\r\n",
        "                               policy_logits=NetworkOutput.build_policy_logits(policy_logits),\r\n",
        "                               hidden_state=hidden_representation[0])\r\n",
        "        return output\r\n",
        "\r\n",
        "    @abstractmethod\r\n",
        "    def _value_transform(self, value: np.array) -> float:\r\n",
        "        pass\r\n",
        "\r\n",
        "    @abstractmethod\r\n",
        "    def _reward_transform(self, reward: np.array) -> float:\r\n",
        "        pass\r\n",
        "\r\n",
        "    @abstractmethod\r\n",
        "    def _conditioned_hidden_state(self, hidden_state: np.array, action: Action) -> np.array:\r\n",
        "        pass\r\n",
        "\r\n",
        "    def cb_get_variables(self) -> Callable:\r\n",
        "        \"\"\"Return a callback that return the trainable variables of the network.\"\"\"\r\n",
        "\r\n",
        "        def get_variables():\r\n",
        "            networks = (self.representation_network, self.value_network, self.policy_network,\r\n",
        "                        self.dynamic_network, self.reward_network)\r\n",
        "            return [variables\r\n",
        "                    for variables_list in map(lambda n: n.weights, networks)\r\n",
        "                    for variables in variables_list]\r\n",
        "\r\n",
        "        return get_variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH-ETawXVxaU"
      },
      "source": [
        "### Sauvegarde du réseau <a class=\"anchor\" id=\"sauvegarde_reseau\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ExcHxtbV3RU"
      },
      "source": [
        "Cette classe permet de sauvegarder les différentes versions du réseau. Elle contient une liste des différents réseaux obtenus au fur et à mesure de l'entraînement et peut y ajouter de nouveaux réseaux mais aussi revenir à un réseau antérieur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXI65vVuV0i_"
      },
      "source": [
        "class SharedStorage(object):\r\n",
        "    \"\"\"Save the different versions of the network.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, network: BaseNetwork, uniform_network: UniformNetwork, optimizer: tf.keras.optimizers):\r\n",
        "        self._networks = {}\r\n",
        "        self.current_network = network\r\n",
        "        self.uniform_network = uniform_network\r\n",
        "        self.optimizer = optimizer\r\n",
        "\r\n",
        "    def latest_network(self) -> AbstractNetwork:\r\n",
        "        if self._networks:\r\n",
        "            return self._networks[max(self._networks.keys())]\r\n",
        "        else:\r\n",
        "            # policy -> uniform, value -> 0, reward -> 0\r\n",
        "            return self.uniform_network\r\n",
        "\r\n",
        "    def save_network(self, step: int, network: BaseNetwork):\r\n",
        "        self._networks[step] = network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWezqXWcSc0w"
      },
      "source": [
        "### Le réseau du CartPole <a class=\"anchor\" id=\"reseau_cartpole\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbCd_CqjVZOP"
      },
      "source": [
        "Cette classe implémente le réseau spécifique au jeu du CartPole, elle s'appuie sur la structure nécessaire pour MuZero en héritant de la classe `BaseNetwork`. On constate que contrairement à l'article les réseaux utilisés ne sont pas des réseaux convolutionnels, notamment pour le réseau de représentation. En effet, le jeu étant en 1D l'apprentissage de sa représentation à partir de son observation peut être effectué directement grâce à des MLP. On note aussi comme dans l'article l'utilisation d'une régularisation L2.\r\n",
        "\r\n",
        "La fonction `_value_transform` fait écho à l'annexe F de l'article. Cette fonction reçoit la value sous la forme d'un vecteur de taille `value_support_size`, la transforme en nombre avec un softmax puis au lieu de renvoyer directement cette valeur il renvoie son image par une fonction $h$. Plutôt que d'utiliser la fonction $h$ de l'article l'implémentation de Johan Gras prend une fonction plus facilement inversible qui revient à prendre le carré de la value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l6hZXwPSmi4"
      },
      "source": [
        "class CartPoleNetwork(BaseNetwork):\r\n",
        "    # Initialisation of Cartpole game specific parameters\r\n",
        "    def __init__(self,\r\n",
        "                 state_size: int,\r\n",
        "                 action_size: int,\r\n",
        "                 representation_size: int,\r\n",
        "                 max_value: int,\r\n",
        "                 hidden_neurons: int = 64,\r\n",
        "                 weight_decay: float = 1e-4,\r\n",
        "                 representation_activation: str = 'tanh',\r\n",
        "                 loaded: bool = False,\r\n",
        "                 representation_network_file = None,\r\n",
        "                 value_network_file = None,\r\n",
        "                 policy_network_file = None,\r\n",
        "                 dynamic_network_file = None,\r\n",
        "                 reward_network_file = None):\r\n",
        "        self.state_size = state_size\r\n",
        "        self.action_size = action_size\r\n",
        "        self.representation_size = representation_size\r\n",
        "        self.value_support_size = math.ceil(math.sqrt(max_value)) + 1\r\n",
        "\r\n",
        "        self.max_value = max_value\r\n",
        "        self.hidden_neurons = hidden_neurons\r\n",
        "        self.weight_decay = weight_decay\r\n",
        "        self.representation_activation = representation_activation\r\n",
        "\r\n",
        "        regularizer = regularizers.l2(weight_decay)\r\n",
        "        # Network structure specification\r\n",
        "        if loaded:\r\n",
        "            representation_network = load_model(representation_network_file)\r\n",
        "            value_network = load_model(value_network_file)\r\n",
        "            policy_network = load_model(policy_network_file)\r\n",
        "            dynamic_network = load_model(dynamic_network_file)\r\n",
        "            reward_network = load_model(reward_network_file)\r\n",
        "\r\n",
        "        else:\r\n",
        "            representation_network = Sequential([Dense(hidden_neurons, activation='relu', kernel_regularizer=regularizer),\r\n",
        "                                                Dense(representation_size, activation=representation_activation,\r\n",
        "                                                      kernel_regularizer=regularizer)])\r\n",
        "            value_network = Sequential([Dense(hidden_neurons, activation='relu', kernel_regularizer=regularizer),\r\n",
        "                                        Dense(self.value_support_size, kernel_regularizer=regularizer)])\r\n",
        "            policy_network = Sequential([Dense(hidden_neurons, activation='relu', kernel_regularizer=regularizer),\r\n",
        "                                        Dense(action_size, kernel_regularizer=regularizer)])\r\n",
        "            dynamic_network = Sequential([Dense(hidden_neurons, activation='relu', kernel_regularizer=regularizer),\r\n",
        "                                          Dense(representation_size, activation=representation_activation,\r\n",
        "                                                kernel_regularizer=regularizer)])\r\n",
        "            reward_network = Sequential([Dense(16, activation='relu', kernel_regularizer=regularizer),\r\n",
        "                                        Dense(1, kernel_regularizer=regularizer)])\r\n",
        "\r\n",
        "        super().__init__(representation_network, value_network, policy_network, dynamic_network, reward_network)\r\n",
        "    #Specification of the abstract functions of the structre of BaseNetwork() used in the functions to get the 5 key quantities \r\n",
        "    def _value_transform(self, value_support: np.array) -> float:\r\n",
        "        \"\"\"\r\n",
        "        The value is obtained by first computing the expected value from the discrete support.\r\n",
        "        Second, the inverse transform is then apply (the square function).\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        value = self._softmax(value_support)\r\n",
        "        value = np.dot(value, range(self.value_support_size))\r\n",
        "        value = value.item() ** 2\r\n",
        "        return value\r\n",
        "\r\n",
        "    def _reward_transform(self, reward: np.array) -> float:\r\n",
        "        return reward.item()\r\n",
        "\r\n",
        "    # concatenate the hidden state and a diagonal matrix representing the action\r\n",
        "    def _conditioned_hidden_state(self, hidden_state: np.array, action: Action) -> np.array:\r\n",
        "        conditioned_hidden = np.concatenate((hidden_state, np.eye(self.action_size)[action.index]))\r\n",
        "        return np.expand_dims(conditioned_hidden, axis=0)\r\n",
        "\r\n",
        "    def _softmax(self, values):\r\n",
        "        \"\"\"Compute softmax using numerical stability tricks.\"\"\"\r\n",
        "        values_exp = np.exp(values - np.max(values))\r\n",
        "        return values_exp / np.sum(values_exp)\r\n",
        "\r\n",
        "    # function that we have added, useful for the save of a trained network\r\n",
        "    def show_basic_attributes(self):\r\n",
        "        return [self.state_size, self.action_size, self.representation_size, self.max_value,\r\n",
        "                self.hidden_neurons, self.weight_decay, self.representation_activation]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHFjvn2R1r-L"
      },
      "source": [
        "### Autre jeu : Acrobot <a class=\"anchor\" id=\"reseau_acrobot\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAF6CMLLTzMw"
      },
      "source": [
        "Le réseau utilisé pour ce jeu peut avoir la même structure que celui du CartPole puisqu'il sont tous deux en 1D. La seule différence est la taille des entrées et des sorties qui sont précisées dans la configuration du jeu. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-ZnUc1V1qkp"
      },
      "source": [
        "class AcrobotNetwork(BaseNetwork):\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 state_size: int,\r\n",
        "                 action_size: int,\r\n",
        "                 representation_size: int,\r\n",
        "                 max_value: int,\r\n",
        "                 hidden_neurons: int = 64,\r\n",
        "                 weight_decay: float = 1e-4,\r\n",
        "                 representation_activation: str = 'tanh'):\r\n",
        "        self.state_size = state_size\r\n",
        "        self.action_size = action_size\r\n",
        "        self.value_support_size = math.ceil(math.sqrt(max_value)) + 1\r\n",
        "\r\n",
        "        regularizer = regularizers.l2(weight_decay)\r\n",
        "        representation_network = Sequential([Dense(hidden_neurons, activation='relu', kernel_regularizer=regularizer),\r\n",
        "                                             Dense(representation_size, activation=representation_activation,\r\n",
        "                                                   kernel_regularizer=regularizer)])\r\n",
        "        value_network = Sequential([Dense(hidden_neurons, activation='relu', kernel_regularizer=regularizer),\r\n",
        "                                    Dense(self.value_support_size, kernel_regularizer=regularizer)])\r\n",
        "        policy_network = Sequential([Dense(hidden_neurons, activation='relu', kernel_regularizer=regularizer),\r\n",
        "                                     Dense(action_size, kernel_regularizer=regularizer)])\r\n",
        "        dynamic_network = Sequential([Dense(hidden_neurons, activation='relu', kernel_regularizer=regularizer),\r\n",
        "                                      Dense(representation_size, activation=representation_activation,\r\n",
        "                                            kernel_regularizer=regularizer)])\r\n",
        "        reward_network = Sequential([Dense(16, activation='relu', kernel_regularizer=regularizer),\r\n",
        "                                     Dense(1, kernel_regularizer=regularizer)])\r\n",
        "\r\n",
        "        super().__init__(representation_network, value_network, policy_network, dynamic_network, reward_network)\r\n",
        "\r\n",
        "    def _value_transform(self, value_support: np.array) -> float:\r\n",
        "        \"\"\"\r\n",
        "        The value is obtained by first computing the expected value from the discrete support.\r\n",
        "        Second, the inverse transform is then apply (the square function).\r\n",
        "        \"\"\"\r\n",
        "        value = self._softmax(value_support)\r\n",
        "        value = np.dot(value, range(self.value_support_size))\r\n",
        "        value = value.item() ** 2\r\n",
        "        # value = np.sign(value.item()) * value.item()**2\r\n",
        "        return value\r\n",
        "\r\n",
        "    def _reward_transform(self, reward: np.array) -> float:\r\n",
        "        return reward.item()\r\n",
        "\r\n",
        "    def _conditioned_hidden_state(self, hidden_state: np.array, action: Action) -> np.array:\r\n",
        "        conditioned_hidden = np.concatenate((hidden_state, np.eye(self.action_size)[action.index]))\r\n",
        "        return np.expand_dims(conditioned_hidden, axis=0)\r\n",
        "\r\n",
        "    def _softmax(self, values):\r\n",
        "        \"\"\"Compute softmax using numerical stability tricks.\"\"\"\r\n",
        "        values_exp = np.exp(values - np.max(values))\r\n",
        "        return values_exp / np.sum(values_exp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exZ4qk10LBtJ"
      },
      "source": [
        "## Configuration pour l'algorithme <a class=\"anchor\" id=\"config\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALazyfXatR6j"
      },
      "source": [
        "Cette classe implémente la configuration nécessaire pour pouvoir exécuter notre algorithme MuZero sur le jeu choisi. On y retrouve tous les hyperparamètres nécessaires à l'entraînement et à l'évaluation, dont les valeurs seront passées juste avant l'exécution de l'algorithme en accord avec l'article et le jeu choisi. \r\n",
        "\r\n",
        "Les hyperparamètres les plus importants sont les suivants:\r\n",
        "- `nb_training_loop`: nombre de boucles d'entraînement. Les parties de jeu sont effectuées au début de chaque boucle d'entraînement avec le dernier réseau sauvegardé.\r\n",
        "- `nb_episodes`: nombre de parties de jeu effectuées pour l'entraînement et sauvegardées dans le `ReplayBuffer` à chaque début de boucle d'entraînement.\r\n",
        "- `nb_epochs`: nombre de mises à jour des poids du réseau au sein de chaque boucle d'entraînement. Pour chaque epoch on échantillonne un batch des parties du `ReplayBuffer` (toutes effectuées avec le réseau disponible au début de la boucle d'entraînement) et on met à jour les paramètres grâce à ces parties. A chaque fin d'epoch le réseau avec ses nouveaux poids est sauvegardé.\r\n",
        "- `nb_eval_episodes`: nombre de parties de jeu effectuées pour l'évaluation. Elles sont réalisées avec le dernier réseau sauvegardé après la méthode à l'issu des `nb_epochs` réalisés pendans une boucle d'entraînement.\r\n",
        "- `max_moves`: nombre maximal d'actions pouvant être réalisées pour chaque partie. La partie s'arrête si le jeu passe dans l'état \"terminé\" ou si ce nombre maximal d'actions est atteint.\r\n",
        "- `num_simulations`: nombre de parcours de chaque MCTS.\r\n",
        "- `td_steps`: nombre d'étapes hypothétiques à considérer pour calculer les targets (notamment la target value)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGN_aSqpIYyV"
      },
      "source": [
        "KnownBounds = collections.namedtuple('KnownBounds', ['min', 'max'])\r\n",
        "\r\n",
        "\r\n",
        "class MuZeroConfig(object):\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 game,\r\n",
        "                 nb_training_loop: int,\r\n",
        "                 nb_episodes: int,\r\n",
        "                 nb_epochs: int,\r\n",
        "                 nb_eval_episodes: int,\r\n",
        "                 network_args: Dict,\r\n",
        "                 network,\r\n",
        "                 action_space_size: int,\r\n",
        "                 max_moves: int,\r\n",
        "                 discount: float,\r\n",
        "                 dirichlet_alpha: float,\r\n",
        "                 num_simulations: int,\r\n",
        "                 batch_size: int,\r\n",
        "                 td_steps: int,\r\n",
        "                 visit_softmax_temperature_fn,\r\n",
        "                 lr: float,\r\n",
        "                 known_bounds: Optional[KnownBounds] = None):\r\n",
        "        \r\n",
        "        ### Environment\r\n",
        "        self.game = game # Load the (environment of) the game to which we are gonna play \r\n",
        "\r\n",
        "        ### Self-Play\r\n",
        "        self.action_space_size = action_space_size # Set the size of the action space \r\n",
        "        # self.num_actors = num_actors (since we play only solo games we don't need the variable num_actors)\r\n",
        "\r\n",
        "        # Set the softmax function used to determine the real action at the end of a tree search during training\r\n",
        "        self.visit_softmax_temperature_fn = visit_softmax_temperature_fn \r\n",
        "        # Set the maximal number of actions that can be played in one game \r\n",
        "        self.max_moves = max_moves\r\n",
        "        # Set the number of times we go through each Monte Carlo Tree\r\n",
        "        self.num_simulations = num_simulations\r\n",
        "        # Set the discount rate (to compute the target value)\r\n",
        "        self.discount = discount\r\n",
        "\r\n",
        "        # Root prior exploration noise.\r\n",
        "        self.root_dirichlet_alpha = dirichlet_alpha\r\n",
        "        self.root_exploration_fraction = 0.25\r\n",
        "\r\n",
        "        # Set the UCB parameters (maximisation of the UCB score allows to select a child node in MCTS)\r\n",
        "        # we take the c values from the appendix B of the article\r\n",
        "        self.pb_c_base = 19652\r\n",
        "        self.pb_c_init = 1.25\r\n",
        "\r\n",
        "        # If we already have some information about which values occur in the\r\n",
        "        # environment, we can use them to initialize the rescaling.\r\n",
        "        # This is not strictly necessary, but establishes identical behaviour to\r\n",
        "        # AlphaZero in board games.\r\n",
        "        self.known_bounds = known_bounds\r\n",
        "\r\n",
        "        ### Training\r\n",
        "        self.nb_training_loop = nb_training_loop # Nb of training loop \r\n",
        "        self.nb_episodes = nb_episodes  # Nb of episodes per training loop (nb of games played by training loop and stored in the replay Buffer cf. see Entrainement part)\r\n",
        "        self.nb_epochs = nb_epochs  # Nb of epochs per training loop \r\n",
        "        \r\n",
        "        ### Evaluation\r\n",
        "        self.nb_eval_episodes = nb_eval_episodes #Nb of games used for the evaluation\r\n",
        "\r\n",
        "        # self.training_steps = int(1000e3)\r\n",
        "        # self.checkpoint_interval = int(1e3)\r\n",
        "        self.window_size = int(1e6)\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.num_unroll_steps = 5\r\n",
        "        self.td_steps = td_steps\r\n",
        "\r\n",
        "        self.weight_decay = 1e-4\r\n",
        "        self.momentum = 0.9\r\n",
        "\r\n",
        "        self.network_args = network_args\r\n",
        "        self.network = network\r\n",
        "        self.lr = lr\r\n",
        "        # Exponential learning rate schedule\r\n",
        "        # self.lr_init = lr_init\r\n",
        "        # self.lr_decay_rate = 0.1\r\n",
        "        # self.lr_decay_steps = lr_decay_steps\r\n",
        "\r\n",
        "    def new_game(self) -> AbstractGame:\r\n",
        "        return self.game(self.discount)\r\n",
        "\r\n",
        "    def new_network(self) -> BaseNetwork:\r\n",
        "        return self.network(**self.network_args)\r\n",
        "\r\n",
        "    def uniform_network(self) -> UniformNetwork:\r\n",
        "        return UniformNetwork(self.action_space_size)\r\n",
        "\r\n",
        "    def new_optimizer(self) -> tf.keras.optimizers:\r\n",
        "        return tf.keras.optimizers.SGD(learning_rate=self.lr, momentum=self.momentum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-trCZj8uYD"
      },
      "source": [
        "On note que la fonction `make_board_game_config` décrite dans le pseudo-code n'est pas implémentée ici puisque le jeu du CartPole ne nécessite pas de plateau."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czSq7xurECiW"
      },
      "source": [
        "## Monte Carlo Tree Search (MCTS) <a class=\"anchor\" id=\"MCTS\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y3ofviUqwjf"
      },
      "source": [
        "Cette section implémente les arbres de recherche de Monte Carlo permettant de sélectionner les actions lors de la simulation de parties de jeu. Les MCTS (arbres de recherche de Monte Carlo) permettent de sélectionner l'action à effectuer selon l'état correspondant au moment du jeu choisi, cela grâce la simulation de K étapes hypothétiques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q11eqTkSrb1E"
      },
      "source": [
        "Cette première classe permet de stocker les values minimales et maximales d'un MCTS et de normaliser chaque valeur grâce à ces dernières en les ramenant entre 0 et 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUjPTOkqVlB1"
      },
      "source": [
        "class MinMaxStats(object):\r\n",
        "    \"\"\"A class that holds the min-max values of the tree.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, known_bounds):\r\n",
        "        self.maximum = known_bounds.max if known_bounds else -MAXIMUM_FLOAT_VALUE\r\n",
        "        self.minimum = known_bounds.min if known_bounds else MAXIMUM_FLOAT_VALUE\r\n",
        "\r\n",
        "    def update(self, value: float):\r\n",
        "        if value is None:\r\n",
        "            raise ValueError\r\n",
        "\r\n",
        "        self.maximum = max(self.maximum, value)\r\n",
        "        self.minimum = min(self.minimum, value)\r\n",
        "\r\n",
        "    def normalize(self, value: float) -> float:\r\n",
        "        # If the value is unknow, by default we set it to the minimum possible value\r\n",
        "        if value is None:\r\n",
        "            return 0.0\r\n",
        "\r\n",
        "        if self.maximum > self.minimum:\r\n",
        "            # We normalize only when we have set the maximum and minimum values.\r\n",
        "            return (value - self.minimum) / (self.maximum - self.minimum)\r\n",
        "        return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckTh-MdtbfxT"
      },
      "source": [
        "La suite de cette section contient l'implémentation des arbres de recherche de Monte Carlo grâce auxquels notre algorithme peut effectuer de la planification. En effet, ces arbres explorent les différentes trajectoires en utilisant les fonctions de dynamique, représentation et prédiction actuelles du modèle puis renvoient une value, policy et reward estimées. La policy estimée grâce au nombre de visites de chaque noeud fils de la racine sera ensuite utlisée pour choisir une action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXx3sxINjPB5"
      },
      "source": [
        "### L'implémentation de la sélection <a class=\"anchor\" id=\"selection\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctp7bpLNeIlV"
      },
      "source": [
        "Cette fonction permet d'ajouter un bruit suivant une loi de Dirichlet à l'a priori de la racine de l'arbre de recherche. Cette technique permet de forcer l'arbre à faire de l'exploration lors de sa recherche."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91s1vSwZEFNR"
      },
      "source": [
        "def add_exploration_noise(config: MuZeroConfig, node: Node):\r\n",
        "    \"\"\"\r\n",
        "    At the start of each search, we add dirichlet noise to the prior of the root\r\n",
        "    to encourage the search to explore new actions.\r\n",
        "    \"\"\"\r\n",
        "    actions = list(node.children.keys())\r\n",
        "    noise = np.random.dirichlet([config.root_dirichlet_alpha] * len(actions))\r\n",
        "    frac = config.root_exploration_fraction\r\n",
        "    for a, n in zip(actions, noise):\r\n",
        "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zalneYcuHKj"
      },
      "source": [
        "Cette fonction permet de sélectionner parmis les fils d'un noeud de l'arbre le prochain vers lequel on va aller et l'action à entreprendre. Si le noeud n'a jamais été visité elle choisit aléatoirement un de ses noeuds fils et l'action y menant, sinon elle choisit celui qui maximise le score UCB et l'action qui y mène."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91wejmPjbRTg"
      },
      "source": [
        "def select_child(config: MuZeroConfig, node: Node, min_max_stats: MinMaxStats):\r\n",
        "    \"\"\"\r\n",
        "    Select the child with the highest UCB score.\r\n",
        "    \"\"\"\r\n",
        "    # When the parent visit count is zero, all ucb scores are zeros, therefore we return a random child\r\n",
        "    if node.visit_count == 0:\r\n",
        "        return random.sample(node.children.items(), 1)[0]\r\n",
        "\r\n",
        "    _, action, child = max(\r\n",
        "        (ucb_score(config, node, child, min_max_stats), action,\r\n",
        "         child) for action, child in node.children.items())\r\n",
        "    return action, child"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SCemVcAvBPi"
      },
      "source": [
        "Cette fonction est essentielle pour le parcours d'un MCTS, elle permet de calculer le score UCB (Uniform Contiuous Bound) d'un noeud et du fils choisi grâce à la formule donnée dans l'annexe B de l'article. Pour ce faire elle calcule la value associée au fils et y ajoute un bonus d'exploration. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eknbZBwJbSH4"
      },
      "source": [
        "def ucb_score(config: MuZeroConfig, parent: Node, child: Node,\r\n",
        "              min_max_stats: MinMaxStats) -> float:\r\n",
        "    \"\"\"\r\n",
        "    The score for a node is based on its value, plus an exploration bonus based on\r\n",
        "    the prior.\r\n",
        "    \"\"\"\r\n",
        "    # See Appendix B of the article for precise formulations (we ddecided not to put it in the report either)\r\n",
        "    pb_c = math.log((parent.visit_count + config.pb_c_base + 1) / config.pb_c_base) + config.pb_c_init\r\n",
        "    pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\r\n",
        "\r\n",
        "    prior_score = pb_c * child.prior\r\n",
        "    value_score = min_max_stats.normalize(child.value())\r\n",
        "    return prior_score + value_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cHdOKcLjtiI"
      },
      "source": [
        "### L'implémentation de l'expansion d'un noeud <a class=\"anchor\" id=\"expansion\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEc-Lmfdw7C6"
      },
      "source": [
        "Cette fonction permet de remplir les attributs d'un noeud à partir des différents réseaux de notre algorithme. On notera notamment `hidden_state` représentant l'état caché correspondant à ce noeud, la reward calculée grâce à la fonction de reward des réseaux mais aussi le remplissage des différentes actions possibles depuis ce noeud grâce à la policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78G7S8sJbUDK"
      },
      "source": [
        "def expand_node(node: Node, to_play: Player, actions: List[Action],\r\n",
        "                network_output: NetworkOutput):\r\n",
        "    \"\"\"\r\n",
        "    We expand a node using the value, reward and policy prediction obtained from\r\n",
        "    the neural networks.\r\n",
        "    \"\"\"\r\n",
        "    node.to_play = to_play\r\n",
        "    node.hidden_state = network_output.hidden_state # We retrieve the hidden_state value corresponding to the actual node thanks to the networks \r\n",
        "                                                    # implemented in RecurrentModel() passed to BaseNetwork() passed to Game specific Network() \r\n",
        "    node.reward = network_output.reward # Same thing for the reward\r\n",
        "    policy = {a: math.exp(network_output.policy_logits[a]) for a in actions} # Same thing for the policies that are calulated thanks to the list of possible actions\r\n",
        "    policy_sum = sum(policy.values())\r\n",
        "    for action, p in policy.items():\r\n",
        "        node.children[action] = Node(p / policy_sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UuWgtLvo4Vj"
      },
      "source": [
        "### L'implémentation de la rétropropagation <a class=\"anchor\" id=\"retropropagation\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKK12fWvx0MK"
      },
      "source": [
        "Cette fonction permet la backpropagation à travers l'arbre. Une fois la simulation terminée (i.e. une feuille atteinte) on fait la backpropagation jusqu'à la racine de l'arbre en remplissant toutes les values en discountant les rewards et en ajoutant +1 au compteur de visite pour chaque noeud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gLqMoGBbXkt"
      },
      "source": [
        "def backpropagate(search_path: List[Node], value: float, to_play: Player,\r\n",
        "                  discount: float, min_max_stats: MinMaxStats):\r\n",
        "    \"\"\"\r\n",
        "    At the end of a simulation, we propagate the evaluation all the way up the\r\n",
        "    tree to the root.\r\n",
        "    \"\"\"\r\n",
        "    for node in search_path[::-1]:\r\n",
        "        node.value_sum += value if node.to_play == to_play else -value\r\n",
        "        node.visit_count += 1\r\n",
        "        min_max_stats.update(node.value())\r\n",
        "\r\n",
        "        value = node.reward + discount * value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CW4PdyVzFSH"
      },
      "source": [
        "Cette fonction permet d'échantilloner une action grâce à un softmax sur les visites de chaque noeud. Plus précisément on regarde les noeuds fils de la racine et leur nombre de visite et on utilise ces nombres de visite pour calculer un softmax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXxb2niTPYdC"
      },
      "source": [
        "def softmax_sample(visit_counts, actions, t):\r\n",
        "    counts_exp = np.exp(visit_counts) * (1 / t)\r\n",
        "    probs = counts_exp / np.sum(counts_exp, axis=0)\r\n",
        "    action_idx = np.random.choice(len(actions), p=probs)\r\n",
        "    return actions[action_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wySr8Ok2pRdt"
      },
      "source": [
        "### Excécution de la recherche MCTS et sélection de l'action réalisée <a class=\"anchor\" id=\"excecution\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTQppLnhehfa"
      },
      "source": [
        "Cette fonction constitue le coeur de l'algorithme MCTS. Elle exécute `num_simulations` simulations (c'est à dire `num_simulations` parcours de l'arbre commençant à la racine et se terminant dès l'atteinte d'une feuille). Les fonctions suivantes implémentent le passage d'un noeud à un autre grâce à la maximisation du score UCB ainsi que la backpropagation qui permet le calcul de la reward associée au parcours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0NECZsYbIoD"
      },
      "source": [
        "def run_mcts(config: MuZeroConfig, root: Node, action_history: ActionHistory, network: BaseNetwork):\r\n",
        "    \"\"\"\r\n",
        "    Core Monte Carlo Tree Search algorithm.\r\n",
        "    To decide on an action, we run N simulations, always starting at the root of\r\n",
        "    the search tree and traversing the tree according to the UCB formula until we\r\n",
        "    reach a leaf node.\r\n",
        "    \"\"\"\r\n",
        "    min_max_stats = MinMaxStats(config.known_bounds)\r\n",
        "\r\n",
        "    for _ in range(config.num_simulations): # Number of simulations (number of times we go through the tree, precised in the excecution configuration)\r\n",
        "        history = action_history.clone()\r\n",
        "        node = root\r\n",
        "        search_path = [node]\r\n",
        "\r\n",
        "        while node.expanded():\r\n",
        "            action, node = select_child(config, node, min_max_stats)\r\n",
        "            history.add_action(action)\r\n",
        "            search_path.append(node)\r\n",
        "\r\n",
        "        # Inside the search tree we use the dynamics function to obtain the next\r\n",
        "        # hidden state given an action and the previous hidden state.\r\n",
        "        parent = search_path[-2]\r\n",
        "        network_output = network.recurrent_inference(parent.hidden_state, history.last_action())\r\n",
        "        expand_node(node, history.to_play(), history.action_space(), network_output)\r\n",
        "\r\n",
        "        backpropagate(search_path, network_output.value, history.to_play(), config.discount, min_max_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNtgnZCEfXDQ"
      },
      "source": [
        "Cette fonction permet de sélectionner une action à la suite des N simulations. Pour ce faire elle se base sur le nombre de visites des noeuds fils de la racine. On remarque qu'elle a 2 modes : un pour l'entraînement où elle s'intéresse à l'exploration et ne sélectionne pas forcément le noeud le plus visité mais utilise plutôt la probabilité donnée par le softmax décrit au dessus, et un pour l'évaluation où le noeud le plus visité est sélectionné."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xzdmzCKbIUc"
      },
      "source": [
        "def select_action(config: MuZeroConfig, num_moves: int, node: Node, network: BaseNetwork, mode: str = 'softmax'):\r\n",
        "    \"\"\"\r\n",
        "    After running simulations inside in MCTS, we select an action based on the root's children visit counts.\r\n",
        "    During training we use a softmax sample for exploration.\r\n",
        "    During evaluation we select the most visited child.\r\n",
        "    \"\"\"\r\n",
        "    visit_counts = [child.visit_count for child in node.children.values()]\r\n",
        "    actions = [action for action in node.children.keys()]\r\n",
        "    action = None\r\n",
        "    if mode == 'softmax':\r\n",
        "        t = config.visit_softmax_temperature_fn(\r\n",
        "            num_moves=num_moves, training_steps=network.training_steps)\r\n",
        "        action = softmax_sample(visit_counts, actions, t)\r\n",
        "    elif mode == 'max':\r\n",
        "        action, _ = max(node.children.items(), key=lambda item: item[1].visit_count)\r\n",
        "    return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpRa8JCO01rq"
      },
      "source": [
        "## Entraînement <a class=\"anchor\" id=\"entrainement\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfEOD-Xy_TsN"
      },
      "source": [
        "Une trajectoire est échantillonée depuis le replay buffer. Pour la première étape, la fonction de représentation du modèle reçoit en entrée les observations passées de la trajectoire sélectionnée. Le modèle est par la suite déroulé de manière récurrente sur K étapes hypothétiques. L'objectif est de minimiser une loss définie dans l'article comme la somme de trois loss : une pour les rewards, une pour la value et une pour la policy. Une étape de backpropagation est effectuée afin de la minimiser et donc d'optimiser les paramètres du réseau."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxXvz4fOWvxR"
      },
      "source": [
        "#### Replay Buffer <a class=\"anchor\" id=\"replay_buffer\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoWcuPRrXEAv"
      },
      "source": [
        "Cette classe, utile pour l'entraînement de l'algorithme, est chargée de toute la gestion des jeux et de leurs sauvegardes. Il faut lui fournir des jeux qu'elle stockera grâce à la fonction `save_game()` et elle peut ensuite renvoyer pour un ensemble aléatoires de parties réalisées un instant aléatoire par partie (via la fonction `sample_position`) dans des batchs grâce à la fonction `sample_batch`. Cette fonction utilise les données d'un ensemble de `batch_size` jeux tirés aléatoirement parmi ceux que le `ReplayBuffer` contient puis extrait les informations nécessaires pour y entraîner les réseaux décrits dans la section précédente.\r\n",
        "\r\n",
        "Afin de fournir des parties jouées au `ReplayBuffer` il faut simuler ces parties en générant des suites d'actions conduisant à la fin d'une partie, c'est ce dont se chargera la partie Self-Play présentée juste après dans le notebook. Cette partie de simulation s'appuie notamment sur les arbres de Monte Carlo et elle est effectuée après chaque étape d'entraînement des différentes réseaux."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8xEvQEVD-Ve"
      },
      "source": [
        "class ReplayBuffer(object):\r\n",
        "\r\n",
        "    def __init__(self, config: MuZeroConfig):\r\n",
        "        self.window_size = config.window_size\r\n",
        "        self.batch_size = config.batch_size\r\n",
        "        self.buffer = []\r\n",
        "\r\n",
        "    def save_game(self, game):\r\n",
        "        if len(self.buffer) > self.window_size:\r\n",
        "            self.buffer.pop(0)\r\n",
        "        self.buffer.append(game)\r\n",
        "\r\n",
        "    def sample_batch(self, num_unroll_steps: int, td_steps: int):\r\n",
        "        # Generate some samples of data to train on\r\n",
        "        games = self.sample_games()\r\n",
        "        game_pos = [(g, self.sample_position(g)) for g in games]\r\n",
        "        game_data = [(g.make_image(i), g.history[i:i + num_unroll_steps],\r\n",
        "                      g.make_target(i, num_unroll_steps, td_steps, g.to_play()))\r\n",
        "                     for (g, i) in game_pos]\r\n",
        "\r\n",
        "        # Pre-process the batch\r\n",
        "        image_batch, actions_time_batch, targets_batch = zip(*game_data)\r\n",
        "        targets_init_batch, *targets_time_batch = zip(*targets_batch)\r\n",
        "        actions_time_batch = list(zip_longest(*actions_time_batch, fillvalue=None))\r\n",
        "\r\n",
        "        # Building batch of valid actions and a dynamic mask for hidden representations during BPTT\r\n",
        "        mask_time_batch = []\r\n",
        "        dynamic_mask_time_batch = []\r\n",
        "        last_mask = [True] * len(image_batch)\r\n",
        "        for i, actions_batch in enumerate(actions_time_batch):\r\n",
        "            mask = list(map(lambda a: bool(a), actions_batch))\r\n",
        "            dynamic_mask = [now for last, now in zip(last_mask, mask) if last]\r\n",
        "            mask_time_batch.append(mask)\r\n",
        "            dynamic_mask_time_batch.append(dynamic_mask)\r\n",
        "            last_mask = mask\r\n",
        "            actions_time_batch[i] = [action.index for action in actions_batch if action]\r\n",
        "        \r\n",
        "        batch = image_batch, targets_init_batch, targets_time_batch, actions_time_batch, mask_time_batch, dynamic_mask_time_batch\r\n",
        "        return batch\r\n",
        "\r\n",
        "    def sample_games(self) -> List[AbstractGame]:\r\n",
        "        # Sample game from buffer either uniformly or according to some priority.\r\n",
        "        return random.choices(self.buffer, k=self.batch_size)\r\n",
        "\r\n",
        "    def sample_position(self, game: AbstractGame) -> int:\r\n",
        "        # Sample position from game either uniformly or according to some priority.\r\n",
        "        return random.randint(0, len(game.history))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59nCbpAcgMyX"
      },
      "source": [
        "### Self-play <a class=\"anchor\" id=\"self-play\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAB2Y1a1gfFt"
      },
      "source": [
        "Cette section contient toutes les fonctions nécessaires pouvant réaliser des parties de jeux pouvant remplir le `ReplayBuffer` et ainsi pouvoir entraîner et évaluer notre modèle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAu3UxIdjhWy"
      },
      "source": [
        "Cette fonction implémente le jeu d'une partie. Elle part d'un état de jeu initial (un plateau dans le cas d'un jeu à plateau et la position et l'angle initial du CartPole pour notre jeu) et génère les mouvements jusqu'à la fin du jeu. Elle exécute donc des MCTS à chaque tour pour simuler les K prochaines étapes hypothétiques et ainsi sélectionner l'action à réaliser, le jeu s'arrêtant lorsque la barre du CartPole tombe. Elle dispose de deux modes : train ou non. Dans le cas de train l'action suivante est sélectionnée avec un softmax et il y a un bruit ajouté à l'a priori de la racine. Dans le cas où on ne train pas, l'action suivante est sélectionnée de manière déterministe selon le nombre de visites des fils de la racine et il n'y a pas de bruit à la racine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkcRbb8lgTH9"
      },
      "source": [
        "def play_game(config: MuZeroConfig, network: AbstractNetwork, train: bool = True) -> AbstractGame:\r\n",
        "    \"\"\"\r\n",
        "    Each game is produced by starting at the initial board position, then\r\n",
        "    repeatedly executing a Monte Carlo Tree Search to generate moves until the end\r\n",
        "    of the game is reached.\r\n",
        "    \"\"\"\r\n",
        "    game = config.new_game()\r\n",
        "    mode_action_select = 'softmax' if train else 'max'\r\n",
        "\r\n",
        "    while not game.terminal() and len(game.history) < config.max_moves:\r\n",
        "        # At the root of the search tree we use the representation function to\r\n",
        "        # obtain a hidden state given the current observation.\r\n",
        "        root = Node(0)\r\n",
        "        current_observation = game.make_image(-1)\r\n",
        "        expand_node(root, game.to_play(), game.legal_actions(), network.initial_inference(current_observation))\r\n",
        "        if train:\r\n",
        "            add_exploration_noise(config, root)\r\n",
        "\r\n",
        "        # We then run a Monte Carlo Tree Search using only action sequences and the\r\n",
        "        # model learned by the networks.\r\n",
        "        run_mcts(config, root, game.action_history(), network)\r\n",
        "        action = select_action(config, len(game.history), root, network, mode=mode_action_select)\r\n",
        "        game.apply(action)\r\n",
        "        game.store_search_statistics(root)\r\n",
        "\r\n",
        "    return game"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "964YinNBhk--"
      },
      "source": [
        "Cette première fonction permet de remplir le `ReplayBuffer` en réalisant des parties de jeux grâce à la fonction précédente en utilisant son mode train. Elle utilise le dernier réseau sauvegardé de notre algorithme pour jouer `train_episodes` fois à un jeu et sauvegarde chaque partie dans le buffer. Pour ces jeux elle utilise le mode \"train\" de notre algorithme avec de l'exploration dans les MCTS (bruit à la racine + action aléatoire avec le softmax). Elle renvoie aussi la moyenne des rewards totales sur les `train_episodes` parties effectuées qui jouera par la suite le rôle de train score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhmBRu9igVP3"
      },
      "source": [
        "def run_selfplay(config: MuZeroConfig, storage: SharedStorage, replay_buffer: ReplayBuffer, train_episodes: int):\r\n",
        "    \"\"\"Take the latest network, produces multiple games and save them in the shared replay buffer\"\"\"\r\n",
        "    network = storage.latest_network()\r\n",
        "    returns = []\r\n",
        "    for _ in range(train_episodes):\r\n",
        "        game = play_game(config, network)\r\n",
        "        replay_buffer.save_game(game)\r\n",
        "        returns.append(sum(game.rewards))\r\n",
        "    return sum(returns) / train_episodes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efiaCZJIitYN"
      },
      "source": [
        "Cette fonction permet de faire jouer notre algorithme à des jeux en mode \"évaluation\", il n'y a donc plus d'exploration et le but est de choisir l'action selon le noeud le plus visité et non plus avec le softmax. Elle renvoie la moyenne des rewards totales sur les `eval_episodes` parties simulées pour l'évaluation qui servira de score d'évaluation. Comme ces parties ne serviront pas pour l'entraînement il n'y a pas besoin des les stocker dans le `ReplayBuffer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSPn2g5XgXIy"
      },
      "source": [
        "def run_eval(config: MuZeroConfig, storage: SharedStorage, eval_episodes: int):\r\n",
        "    \"\"\"Evaluate MuZero without noise added to the prior of the root and without softmax action selection\"\"\"\r\n",
        "    network = storage.latest_network()\r\n",
        "    returns = []\r\n",
        "    for _ in range(eval_episodes):\r\n",
        "        game = play_game(config, network, train=False)\r\n",
        "        returns.append(sum(game.rewards))\r\n",
        "    return sum(returns) / eval_episodes if eval_episodes else 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo1XY-z2Yzfr"
      },
      "source": [
        "### Training  <a class=\"anchor\" id=\"training\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRNU6HnyZA8p"
      },
      "source": [
        "Les 3 fonctions de cette section permettent d'entraîner le réseau de neurones de l'algorithme MuZero. La loss utilisée est décrite dans l'article dans la section \"3. MuZero Algorithm\", c'est une somme des loss pour la value, la policy et la reward. L'implémentation nous en apprend plus sur les loss utilisées pour chaque composante :\r\n",
        "- une loss personnalisée se basant sur la softmax cross-entropie pour la value\r\n",
        "- une MSE pour les rewards\r\n",
        "- une softmax cross-entropie pour la policy\r\n",
        "\r\n",
        "Les paramètres des réseaux sont entraînés de manière classique grâce à la backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W51V-JE9YxpE"
      },
      "source": [
        "def loss_value(target_value_batch, value_batch, value_support_size: int):\r\n",
        "    batch_size = len(target_value_batch)\r\n",
        "    targets = np.zeros((batch_size, value_support_size))\r\n",
        "    \r\n",
        "    # tentatives de changement de fonction réciproque pour retrouver la value dans le cas Acrobot\r\n",
        "    # sqrt_value = np.sign(target_value_batch) * np.sqrt(np.abs(target_value_batch))\r\n",
        "    # sqrt_value = np.sqrt(np.abs(target_value_batch))\r\n",
        "\r\n",
        "    sqrt_value = np.sqrt(target_value_batch) # on récupère la racine qui correspond à l'inverse de la fonction h appliquée dans _value_transform\r\n",
        "    floor_value = np.floor(sqrt_value).astype(int)\r\n",
        "    rest = sqrt_value - floor_value\r\n",
        "\r\n",
        "    targets[range(batch_size), floor_value.astype(int)] = 1 - rest\r\n",
        "    targets[range(batch_size), floor_value.astype(int) + 1] = rest\r\n",
        "\r\n",
        "    return tf.nn.softmax_cross_entropy_with_logits(logits=value_batch, labels=targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWxV_l0EYok6"
      },
      "source": [
        "def update_weights(optimizer: tf.keras.optimizers, network: BaseNetwork, batch):\r\n",
        "    def scale_gradient(tensor, scale: float):\r\n",
        "        \"\"\"Trick function to scale the gradient in tensorflow\"\"\"\r\n",
        "        return (1. - scale) * tf.stop_gradient(tensor) + scale * tensor\r\n",
        "\r\n",
        "    def loss():\r\n",
        "        loss = 0\r\n",
        "        image_batch, targets_init_batch, targets_time_batch, actions_time_batch, mask_time_batch, dynamic_mask_time_batch = batch\r\n",
        "\r\n",
        "        # Initial step, from the real observation: representation + prediction networks\r\n",
        "        representation_batch, value_batch, policy_batch = network.initial_model(np.array(image_batch))\r\n",
        "\r\n",
        "        # Only update the element with a policy target\r\n",
        "        target_value_batch, _, target_policy_batch = zip(*targets_init_batch)\r\n",
        "        mask_policy = list(map(lambda l: bool(l), target_policy_batch))\r\n",
        "        target_policy_batch = list(filter(lambda l: bool(l), target_policy_batch))\r\n",
        "        policy_batch = tf.boolean_mask(policy_batch, mask_policy)\r\n",
        "\r\n",
        "        # Compute the loss of the first pass\r\n",
        "        loss += tf.math.reduce_mean(loss_value(target_value_batch, value_batch, network.value_support_size))\r\n",
        "        loss += tf.math.reduce_mean(\r\n",
        "            tf.nn.softmax_cross_entropy_with_logits(logits=policy_batch, labels=target_policy_batch))\r\n",
        "\r\n",
        "        # Recurrent steps, from action and previous hidden state.\r\n",
        "        for actions_batch, targets_batch, mask, dynamic_mask in zip(actions_time_batch, targets_time_batch,\r\n",
        "                                                                    mask_time_batch, dynamic_mask_time_batch):\r\n",
        "            target_value_batch, target_reward_batch, target_policy_batch = zip(*targets_batch)\r\n",
        "\r\n",
        "            # Only execute BPTT for elements with an action\r\n",
        "            representation_batch = tf.boolean_mask(representation_batch, dynamic_mask)\r\n",
        "            target_value_batch = tf.boolean_mask(target_value_batch, mask)\r\n",
        "            target_reward_batch = tf.boolean_mask(target_reward_batch, mask)\r\n",
        "            # Creating conditioned_representation: concatenate representations with actions batch\r\n",
        "            actions_batch = tf.one_hot(actions_batch, network.action_size)\r\n",
        "\r\n",
        "            # Recurrent step from conditioned representation: recurrent + prediction networks\r\n",
        "            conditioned_representation_batch = tf.concat((representation_batch, actions_batch), axis=1)\r\n",
        "            representation_batch, reward_batch, value_batch, policy_batch = network.recurrent_model(\r\n",
        "                conditioned_representation_batch)\r\n",
        "\r\n",
        "            # Only execute BPTT for elements with a policy target\r\n",
        "            target_policy_batch = [policy for policy, b in zip(target_policy_batch, mask) if b]\r\n",
        "            mask_policy = list(map(lambda l: bool(l), target_policy_batch))\r\n",
        "            target_policy_batch = tf.convert_to_tensor([policy for policy in target_policy_batch if policy])\r\n",
        "            policy_batch = tf.boolean_mask(policy_batch, mask_policy)\r\n",
        "\r\n",
        "            # Compute the partial loss\r\n",
        "            l = (tf.math.reduce_mean(loss_value(target_value_batch, value_batch, network.value_support_size)) +\r\n",
        "                 MSE(target_reward_batch, tf.squeeze(reward_batch)) +\r\n",
        "                 tf.math.reduce_mean(\r\n",
        "                     tf.nn.softmax_cross_entropy_with_logits(logits=policy_batch, labels=target_policy_batch)))\r\n",
        "\r\n",
        "            # Scale the gradient of the loss by the average number of actions unrolled\r\n",
        "            gradient_scale = 1. / len(actions_time_batch)\r\n",
        "            loss += scale_gradient(l, gradient_scale)\r\n",
        "\r\n",
        "            # Half the gradient of the representation\r\n",
        "            representation_batch = scale_gradient(representation_batch, 0.5)\r\n",
        "\r\n",
        "        return loss\r\n",
        "\r\n",
        "    optimizer.minimize(loss=loss, var_list=network.cb_get_variables())\r\n",
        "    network.training_steps += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9YKXqL6Ylvp"
      },
      "source": [
        "def train_network(config: MuZeroConfig, storage: SharedStorage, replay_buffer: ReplayBuffer, epochs: int):\r\n",
        "    network = storage.current_network\r\n",
        "    optimizer = storage.optimizer\r\n",
        "\r\n",
        "    for _ in range(epochs):\r\n",
        "        batch = replay_buffer.sample_batch(config.num_unroll_steps, config.td_steps)\r\n",
        "        update_weights(optimizer, network, batch)\r\n",
        "        storage.save_network(network.training_steps, network) # the trained network is saved in the shared storage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWig4x6KEKeF"
      },
      "source": [
        "## MuZero <a class=\"anchor\" id=\"muzero\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V83YW163zH_"
      },
      "source": [
        "Nous rappelons que l'algorithme MuZero est basé sur l'utilisation de trois composantes connectées, à savoir : une fonction de représentation, une fonction de dynamique ainsi qu'une fonction de prédiction. Ces trois fonctions sont entrainées de manière parallèle à l'intérieur du réseau.\r\n",
        "\r\n",
        "MuZero utilise un algorithme de recherche basé sur une méthode d'arbre de recherche de Monte-Carlo. Cette méthode permet la convergence de l'algorithme vers la politique optimale pour les jeux à un seul agent et vers la minimax value fonction pour les jeux à somme nulle.\r\n",
        "\r\n",
        "Pour décider d'une action, nous effectuons N parcours de MCTS, en commençant toujours par la racine de l'arbre de recherche et en le parcourant en maximisant le score UCB jusqu'à atteindre une feuille. Pour un noeud donné, le score est donné par sa valeur, avec éventuellement un bonus d'exploration basé sur le prior. Chacun des noeuds contient des informations remplies à partir des fonctions de valeur, de prediction et de reward obtenus à partir du réseau.\r\n",
        "\r\n",
        "A la base de l'arbre de recherche, nous utilisons la fonction de représentation pour obtenir un état caché compte tenu de l'observation actuelle du jeu. Nous appliquons la méthode de recherche d'arbre de Monte Carlo en utilisant uniquement des séquences d'actions et le modèle appris par le réseau. A l'intérieur de l'arbre de recherche nous utilisons la fonction de dynamique pour obtenir le prochain état caché avec l'action et le précédent état caché donnés.\r\n",
        "\r\n",
        "Cette recherche est effectuée à chaque étape t. Une action est échantillonée à partir de la politique de recherche, qui est proprotionnelle au nombre de visites de chaque action à partir de chaque noeud racine. L'environnement reçoit ensuite l'action et génère de nouvelles observations et une nouvelle reward. La trajectoire est ensuite stockée dans un `ReplayBuffer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee3LmNVPELsl"
      },
      "source": [
        "def muzero(config: MuZeroConfig):\r\n",
        "    \"\"\"\r\n",
        "    MuZero training is split into two independent parts: Network training and\r\n",
        "    self-play data generation.\r\n",
        "    These two parts only communicate by transferring the latest networks checkpoint\r\n",
        "    from the training to the self-play, and the finished games from the self-play\r\n",
        "    to the training.\r\n",
        "    In contrast to the original MuZero algorithm this version doesn't works with\r\n",
        "    multiple threads, therefore the training and self-play is done alternately.\r\n",
        "    \"\"\"\r\n",
        "    storage = SharedStorage(config.new_network(), config.uniform_network(), config.new_optimizer())\r\n",
        "    replay_buffer = ReplayBuffer(config)\r\n",
        "\r\n",
        "    train_score = []\r\n",
        "    eval_score = []\r\n",
        "\r\n",
        "    for loop in range(config.nb_training_loop):\r\n",
        "        print(\"Training loop\", loop)\r\n",
        "        score_train = run_selfplay(config, storage, replay_buffer, config.nb_episodes)\r\n",
        "        train_network(config, storage, replay_buffer, config.nb_epochs)\r\n",
        "        score_eval = run_eval(config, storage, config.nb_eval_episodes)\r\n",
        "\r\n",
        "        print(\"Train score:\", score_train)\r\n",
        "        print(\"Eval score:\", score_eval)  # par défaut on simule 50 parties pour l'évaluation\r\n",
        "        print(f\"MuZero played {config.nb_episodes * (loop + 1)} \"\r\n",
        "              f\"episodes and trained for {config.nb_epochs * (loop + 1)} epochs.\\n\")\r\n",
        "\r\n",
        "        train_score.append(score_train)\r\n",
        "        eval_score.append(score_eval)\r\n",
        "\r\n",
        "    return storage.latest_network(), train_score, eval_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnJOkoPpAx3S"
      },
      "source": [
        "## Sauvegarde / Chargement d'un réseau entraîné <a class=\"anchor\" id=\"reseau_entraine\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOTP1Qv_BUHb"
      },
      "source": [
        "Cette section est un ajout par rapport au code présent sur le dépôt Git utilisé. Les deux fonctions suivantes permettent de sauvegarder et de recharger un réseau déjà entraîné. Ces fonctions sont adaptées pour une exécution sur Google Colab, ainsi la première cellule permet de se connecter au compte Google Drive choisi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCHrA-7AA3vT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17e1cbf0-39df-4628-a463-c5eb192eec80"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B4hmRBBBxIE"
      },
      "source": [
        "La fonction de sauvegarde prend en argument un réseau déjà entraîné et renvoyé lors de l'exécution de la fonction `muzero()`. Le deuxième argument est le dossier choisi pour la sauvegarde. A la fin de l'exécution on retrouvera quelques dossiers et un fichier pickle à l'intérieur du dossier choisi, ces différents éléments correspondant aux réseaux et leur paramètres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46LNDUqx-OhJ"
      },
      "source": [
        "def save_network(network, directory):\r\n",
        "    myobject = '-'.join([str(x) for x in network.show_basic_attributes()])\r\n",
        "    with open(directory + '/RL_cartpole_network_basic_attributes.pkl', 'wb') as f:\r\n",
        "        pickle.dump(myobject, f)\r\n",
        "    \r\n",
        "    network.representation_network.save(directory + '/representation_network')\r\n",
        "    network.value_network.save(directory + '/value_network')\r\n",
        "    network.policy_network.save(directory + '/policy_network')\r\n",
        "    network.dynamic_network.save(directory + '/dynamic_network')\r\n",
        "    network.reward_network.save(directory + '/reward_network')\r\n",
        "\r\n",
        "    print(\"Network saved in \" + directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmjfa6WICPdL"
      },
      "source": [
        "La fonction de chargement d'un réseau prend en argument le dossier où les différents éléments du réseau ont été sauvegardés et renvoie un dictionnaire à passer dans l'argument `network_args` de la configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23-ux-iS_zVe"
      },
      "source": [
        "def get_loaded_network_args(directory):\r\n",
        "    representation_network_file = directory + '/representation_network'\r\n",
        "    value_network_file = directory + '/value_network'\r\n",
        "    policy_network_file = directory + '/policy_network'\r\n",
        "    dynamic_network_file = directory + '/dynamic_network'\r\n",
        "    reward_network_file = directory + '/reward_network'\r\n",
        "\r\n",
        "    basic_attributes = pickle.load(open(directory + '/RL_cartpole_network_basic_attributes.pkl', 'rb')).split('-')\r\n",
        "\r\n",
        "    return {'action_size': int(basic_attributes[1]),\r\n",
        "            'state_size': int(basic_attributes[0]),\r\n",
        "            'representation_size': int(basic_attributes[2]),\r\n",
        "            'max_value': int(basic_attributes[3]),\r\n",
        "            'hidden_neurons': int(basic_attributes[4]),\r\n",
        "            'weight_decay': float(basic_attributes[5]),\r\n",
        "            'representation_activation': basic_attributes[6],\r\n",
        "            'loaded': True,\r\n",
        "            'representation_network_file': representation_network_file,\r\n",
        "            'value_network_file': value_network_file,\r\n",
        "            'policy_network_file': policy_network_file,\r\n",
        "            'dynamic_network_file': dynamic_network_file,\r\n",
        "            'reward_network_file': reward_network_file}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ivR-lzyp3U2"
      },
      "source": [
        "## Exécution de MuZero <a class=\"anchor\" id=\"exec_muzero\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYUiunJb2evf"
      },
      "source": [
        "### Execution pour CartPole <a class=\"anchor\" id=\"exec_cartpole\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKm8kSqJomnR"
      },
      "source": [
        "#### Entraînement from scratch <a class=\"anchor\" id=\"from_scratch\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3A3A59sCsE-"
      },
      "source": [
        "Entraînement d'un réseau from scratch et visualisation des différentes rewards obtenues pendant l'entraînement.\r\n",
        "Dans un premier temps on entraîne un réseau pour le jeu du CartPole from scratch pendant 15 boucles d'entraînement (au delà le temps d'entraînement devient extrêmement long). Il est important de noter que l'objectif de l'entraînement est d'obtenir l'Eval score le plus élevée possible, celui-ci correspondant à la moyenne des rewards obtenues par jeu sur les `nb_eval_episodes` parties effectuées pendant une phase d'évaluation.\r\n",
        "\r\n",
        "Dans un premier temps on définit une configuration adaptée à notre comportement avec tous les hyperparamètres introduits auparavant. Ensuite on exécute MuZero sur cette configuration en gardant le dernier réseau et les scores obtenus pendant les différentes boucles d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIv4TTXGtG-T"
      },
      "source": [
        "# configuration pour 12 boucles d'entraînement\r\n",
        "def make_cartpole_config() -> MuZeroConfig:\r\n",
        "    def visit_softmax_temperature(num_moves, training_steps):\r\n",
        "        return 1.0\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    Cette config peut être modifée pour ajuster le nombre d'épisodes \r\n",
        "    d'entraînement et d'évaluation ainsi que tous les autres hyperparamètres.\r\n",
        "    \"\"\"\r\n",
        "    return MuZeroConfig(\r\n",
        "        game=CartPole,\r\n",
        "        nb_training_loop=12,\r\n",
        "        nb_episodes=15,\r\n",
        "        nb_epochs=15,\r\n",
        "        nb_eval_episodes=20,\r\n",
        "        network_args={'action_size': 2,\r\n",
        "                      'state_size': 4,\r\n",
        "                      'representation_size': 4,\r\n",
        "                      'max_value': 500},\r\n",
        "        network=CartPoleNetwork,\r\n",
        "        action_space_size=2,\r\n",
        "        max_moves=1000,\r\n",
        "        discount=0.99,\r\n",
        "        dirichlet_alpha=0.25,\r\n",
        "        num_simulations=11,  # Odd number perform better in eval mode\r\n",
        "        batch_size=512,\r\n",
        "        td_steps=10,\r\n",
        "        visit_softmax_temperature_fn=visit_softmax_temperature,\r\n",
        "        lr=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKnmU76yNC9K",
        "outputId": "774e8a7b-f4bd-4057-83dd-c06fb4eca52d"
      },
      "source": [
        "config = make_cartpole_config()\r\n",
        "muzero_last_network_cartpole, train_score_cartpole, eval_score_cartpole = muzero(config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loop 0\n",
            "Train score: 22.933333333333334\n",
            "Eval score: 9.4\n",
            "MuZero played 15 episodes and trained for 15 epochs.\n",
            "\n",
            "Training loop 1\n",
            "Train score: 19.133333333333333\n",
            "Eval score: 9.7\n",
            "MuZero played 30 episodes and trained for 30 epochs.\n",
            "\n",
            "Training loop 2\n",
            "Train score: 19.266666666666666\n",
            "Eval score: 9.5\n",
            "MuZero played 45 episodes and trained for 45 epochs.\n",
            "\n",
            "Training loop 3\n",
            "Train score: 18.266666666666666\n",
            "Eval score: 9.25\n",
            "MuZero played 60 episodes and trained for 60 epochs.\n",
            "\n",
            "Training loop 4\n",
            "Train score: 16.0\n",
            "Eval score: 13.35\n",
            "MuZero played 75 episodes and trained for 75 epochs.\n",
            "\n",
            "Training loop 5\n",
            "Train score: 21.6\n",
            "Eval score: 20.5\n",
            "MuZero played 90 episodes and trained for 90 epochs.\n",
            "\n",
            "Training loop 6\n",
            "Train score: 27.666666666666668\n",
            "Eval score: 85.65\n",
            "MuZero played 105 episodes and trained for 105 epochs.\n",
            "\n",
            "Training loop 7\n",
            "Train score: 39.333333333333336\n",
            "Eval score: 105.15\n",
            "MuZero played 120 episodes and trained for 120 epochs.\n",
            "\n",
            "Training loop 8\n",
            "Train score: 77.06666666666666\n",
            "Eval score: 21.6\n",
            "MuZero played 135 episodes and trained for 135 epochs.\n",
            "\n",
            "Training loop 9\n",
            "Train score: 27.533333333333335\n",
            "Eval score: 45.9\n",
            "MuZero played 150 episodes and trained for 150 epochs.\n",
            "\n",
            "Training loop 10\n",
            "Train score: 49.06666666666667\n",
            "Eval score: 92.5\n",
            "MuZero played 165 episodes and trained for 165 epochs.\n",
            "\n",
            "Training loop 11\n",
            "Train score: 82.33333333333333\n",
            "Eval score: 98.25\n",
            "MuZero played 180 episodes and trained for 180 epochs.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe529np8ulbv"
      },
      "source": [
        "Une fois l'entraînement terminé on peut visualiser les scores de train et d'évaluation obtenus à chaque boucle d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "aq_-AaK8NGxQ",
        "outputId": "3d54f57d-7095-4c5a-dba8-dff87c40e1ca"
      },
      "source": [
        "plt.plot(train_score_cartpole, label=\"Train score\")\r\n",
        "plt.plot(eval_score_cartpole, label=\"Eval score\")\r\n",
        "plt.title(\"Train and eval score for the CartPole game\")\r\n",
        "plt.xlabel(\"Training loop\")\r\n",
        "plt.ylabel(\"Score\")\r\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f0616f54cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMmOYDs8uyGt"
      },
      "source": [
        "#### Exemple de sauvegarde et chargement du réseau obtenu <a class=\"anchor\" id=\"ex_save\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oye3Zyd1vhSe"
      },
      "source": [
        "##### Sauvegarde <a class=\"anchor\" id=\"sauvegarde\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3dqJcYyNdXh",
        "outputId": "2d2ba08c-dbc3-44c4-af1d-9d3a8866fa6e"
      },
      "source": [
        "# sauvegarde du réseau\r\n",
        "save_network(muzero_last_network_cartpole, '/content/gdrive/My Drive/RL/cartpole_12loops')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/RL/cartpole_15loops/representation_network/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/RL/cartpole_15loops/value_network/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/RL/cartpole_15loops/policy_network/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/RL/cartpole_15loops/dynamic_network/assets\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/RL/cartpole_15loops/reward_network/assets\n",
            "Network saved in /content/gdrive/My Drive/RL/cartpole_15loops\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrbb8tZevT40"
      },
      "source": [
        "Ici on sauvegarde aussi les différents scores obtenus pendant l'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIjhUMEQOQwO"
      },
      "source": [
        "myobject_train = '-'.join([str(x) for x in train_score_cartpole])\r\n",
        "myobject_eval = '-'.join([str(x) for x in eval_score_cartpole])\r\n",
        "\r\n",
        "with open('/content/gdrive/My Drive/RL/cartpole_12loops/train_score.pkl', 'wb') as f:\r\n",
        "    pickle.dump(myobject_train, f)\r\n",
        "with open('/content/gdrive/My Drive/RL/cartpole_12loops/eval_score.pkl', 'wb') as f:\r\n",
        "    pickle.dump(myobject_eval, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0O_9yLOG0do"
      },
      "source": [
        "##### Chargement <a class=\"anchor\" id=\"chargement\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlPmSBGLwB0J"
      },
      "source": [
        "Comme la fonction de chargement renvoie un dictionnaire à passer dans l'argument `network_args` d'une configuration on récupère dans un premier temps ce dictionnaire et on définit une configuration correspondant à notre réseau qui pourra ensuite être réutilisée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxGwrCc1wR5L"
      },
      "source": [
        "network_args_cartpole_12loops = get_loaded_network_args('/content/gdrive/My Drive/RL/cartpole_12loops')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvF5-WRyoZz6"
      },
      "source": [
        "def make_cartpole_config_loaded() -> MuZeroConfig:\r\n",
        "    def visit_softmax_temperature(num_moves, training_steps):\r\n",
        "        return 1.0\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    Cette config peut être modifée pour ajuster le nombre d'épisodes \r\n",
        "    d'entraînement et d'évaluation ainsi que tous les autres hyperparamètres.\r\n",
        "    \"\"\"\r\n",
        "    return MuZeroConfig(\r\n",
        "        game=CartPole,\r\n",
        "        nb_training_loop=2,\r\n",
        "        nb_episodes=15,\r\n",
        "        nb_epochs=15,\r\n",
        "        nb_eval_episodes=20,\r\n",
        "        network_args=network_args_cartpole_12loops, # on passe en argument le réseau obtenu ici\r\n",
        "        network=CartPoleNetwork,\r\n",
        "        action_space_size=2,\r\n",
        "        max_moves=1000,\r\n",
        "        discount=0.99,\r\n",
        "        dirichlet_alpha=0.25,\r\n",
        "        num_simulations=11,  # Odd number perform better in eval mode\r\n",
        "        batch_size=512,\r\n",
        "        td_steps=10,\r\n",
        "        visit_softmax_temperature_fn=visit_softmax_temperature,\r\n",
        "        lr=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKwhT7-AH4Pl"
      },
      "source": [
        "#### Résultats pour plusieurs entraînements <a class=\"anchor\" id=\"resultats\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfWl89RmMtYF"
      },
      "source": [
        "Voici un graphe présentant les scores de train et d'évaluation obtenus pour 3 entraînement comportant 12 boucles d'entraînement chacun."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1F--VFuH_0A"
      },
      "source": [
        "train_scores_cartpole = np.array([[22.93, 19.13, 19.27, 18.27, 16, 21.6, 27.67, 39.33, 77.07, 27.53, 49.07, 82.33],\r\n",
        "                         [22.13, 14.53, 26.4, 18.73, 18.2, 14.87, 10.67, 32.33, 38, 40.87, 49.07, 85.2],\r\n",
        "                         [19.27, 18.73, 16.8, 19.47, 20.8, 18.13, 20.67, 13.87, 17.07, 82.07, 60.13, 110.27]])\r\n",
        "\r\n",
        "eval_scores_cartpole = np.array([[9.4, 9.7, 9.5, 9.25, 13.35, 20.5, 85.65, 105.15, 21.6, 45.9, 92.5, 98.25],\r\n",
        "                                 [9.55, 9.1, 8.95, 9.15, 9.6, 52.8, 43.65, 29.55, 43.25, 90, 95.7, 207.95],\r\n",
        "                                 [9.4, 9.35, 40.6, 9.4, 9.05, 9.35, 9.2, 34.3, 125.05, 56.85, 76.8, 122.8]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "w7ktcv72JX3T",
        "outputId": "ec1951ee-5d80-429c-aa29-58d7fe83f12e"
      },
      "source": [
        "plt.figure(figsize=(15,7))\r\n",
        "plt.plot(np.mean(train_scores_cartpole, axis=0), label=\"Train score\")\r\n",
        "plt.plot(np.mean(eval_scores_cartpole, axis=0), label=\"Eval score\")\r\n",
        "plt.title(\"Mean of train and eval scores for the CartPole game for 3 executions\")\r\n",
        "plt.xlabel(\"Training loop\")\r\n",
        "plt.ylabel(\"Score\")\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOXmXeIWKAmc"
      },
      "source": [
        "Le score d'entraînement et le score d'évaluation obtenus sont la moyenne des récompenses obtenues pour chaque partie de CartPole en mode train ou en mode évaluation. Pour le jeu du CartPole chaque tour réussi entraîne une récompense de +1 et le jeu s'arrête lorsque la barre tombe. Par exemple pour une partie durant 70 tours et pour laquelle la barre tombe au 71ème tour, la récompense totale obtenue sera de +70.\r\n",
        "\r\n",
        "Au cours d'une boucle d'entraînement, le score d'entraînement est obtenu au début grâce aux parties réalisées avec le dernier réseau sauvegardé de la dernière boucle avec de l'exploration dans les MCTS (i.e. avec un bruit de Dirichlet à la racine de ceux-ci). Le score d'évaluation est quant à lui obtenu avec le réseau obtenu à la fin des epochs de la boucle et sans exploration dans les MCTS. Sur ce graphe on observe la moyenne des scores obtenus pour 3 entraînements de MuZero sur le jeu du CartPole afin de vraiment pouvoir observer une tendance plutôt que les particularités d'un seul entraînement.\r\n",
        "\r\n",
        "De manière globale les scores d'évaluation et de train augmentent ce qui montre que l'algorithme apprend. Le train score est constant autour de 20 pendant les 6 premières boucles d'entraînement puis augmente de plus en plus vite pour atteindre 100 après 12 boucles d'entraînement. Différement, l'eval score est presque constant autour de 10 pendant les 4 premières boucles puis augmente ensuite de plus en plus rapidement pour atteindre 140 après 12 boucles d'entraînement. \r\n",
        "\r\n",
        "L'eval score étant obtenu avec un réseau ultérieur donc plus entraîné que celui utilisé pour le train score on pouvait s'attendre à une courbe orange globalement au dessus de la bleue. Cependant, on constate que ceci est faux pour les 4-5 premières boucles d'entraînement. Cela peut s'expliquer en mode évaluation l'algorithme ne fait que de l'exploitation et plus d'exploration, il aurait donc tendance à se focaliser sur des politiques cupides et donc non optimales lorsque le réseau est peu entraîné. A l'inverse, si l'exploration avantage le train score pour les 4-5 premières boucles celle-ci elle ne permet pas de trouver de politque optimale pour la suite de l'entraînement et l'eval score est donc plus élevé.\r\n",
        "\r\n",
        "Il est important de noter que nous n'avons réalisé que 12 boucles d'entraînement afin de pouvoir fournir un graphe moyenné sur plusieurs entraînements. En effet, au delà le temps de calcul devient vraiment long (plus la boucle d'entraînement est avancée et plus le temps d'exécution d'une boucle est long)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44FCq2icGvOH"
      },
      "source": [
        "### Exécution pour Acrobot <a class=\"anchor\" id=\"exec_acrobot\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNK5retEJZnS"
      },
      "source": [
        "Cette configuration correspond au jeu Acrobot et contient quelques différence par rapport à celle du CartPole. En effet on constate que les valeurs des différentes clés du paramètre `network_args` sont différentes pour s'adapter à la dimension des observations et les actions.\r\n",
        "\r\n",
        "Malheureusement nous n'avons pas réussi à faire fonctionner l'implémentation pour ce jeu très probablement à cause de la possibilité d'une reward négative. L'algorithme a bloqué plusieurs fois pour le calcul de la loss en essayant de calculer la réciproque de la fonction $h$ de la value (qui peut être négative). Pour palier à ce phénomène nous avons tenté d'ajouter des valeurs absolues et des facteurs de signe, des valeurs absolues seules, ou même d'ajouter 1 à la reward mais l'algorithme ne s'entraîne pas (tous les scores nuls et le maximum de mouvements d'un jeu ateint à chaque fois). Nous avons préféré laissé ce que nous avions fait pour ce jeu afin de garder toutes les traces de nos essais d'ajout de jeu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y72g9RhZm08n"
      },
      "source": [
        "def make_acrobot_config() -> MuZeroConfig:\r\n",
        "    def visit_softmax_temperature(num_moves, training_steps):\r\n",
        "        return 1.0\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    Cette config peut être modifée pour ajuster le nombre d'épisodes \r\n",
        "    d'entraînement et d'évaluation ainsi que tous les autres hyperparamètres.\r\n",
        "    \"\"\"\r\n",
        "    return MuZeroConfig(\r\n",
        "        game=Acrobot,\r\n",
        "        nb_training_loop=10,\r\n",
        "        nb_episodes=15,\r\n",
        "        nb_epochs=15,\r\n",
        "        nb_eval_episodes=20,\r\n",
        "        network_args={'action_size': 3,\r\n",
        "                      'state_size': 6,\r\n",
        "                      'representation_size': 6,\r\n",
        "                      'max_value': 500},\r\n",
        "        network=AcrobotNetwork,\r\n",
        "        action_space_size=3,\r\n",
        "        max_moves=500,\r\n",
        "        discount=0.99,\r\n",
        "        dirichlet_alpha=0.25,\r\n",
        "        num_simulations=11,  # Odd number perform better in eval mode\r\n",
        "        batch_size=512,\r\n",
        "        td_steps=10,\r\n",
        "        visit_softmax_temperature_fn=visit_softmax_temperature,\r\n",
        "        lr=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "ctErMiKFm0vV",
        "outputId": "aee098c2-a1bb-4861-a5bd-861a6067727e"
      },
      "source": [
        "config_acrobot = make_acrobot_config()\r\n",
        "muzero_last_network_acrobot = muzero(config_acrobot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loop 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-298-e1d8a4afc50d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig_acrobot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_acrobot_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmuzero_last_network_acrobot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmuzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_acrobot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-293-3f730f24239a>\u001b[0m in \u001b[0;36mmuzero\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mscore_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_selfplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mscore_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_eval_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train score:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-292-213ed3e571c2>\u001b[0m in \u001b[0;36mrun_eval\u001b[0;34m(config, storage, eval_episodes)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mreturns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mreturns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0meval_episodes\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0meval_episodes\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-290-54461acb7af1>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(config, network, train)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# We then run a Monte Carlo Tree Search using only action sequences and the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# model learned by the networks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mrun_mcts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode_action_select\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-288-b353c202570e>\u001b[0m in \u001b[0;36mrun_mcts\u001b[0;34m(config, root, action_history, network)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# hidden state given an action and the previous hidden state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mnetwork_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mexpand_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-271-aa4fc4bec949>\u001b[0m in \u001b[0;36mrecurrent_inference\u001b[0;34m(self, hidden_state, action)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mconditioned_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conditioned_hidden_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mhidden_representation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconditioned_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         output = NetworkOutput(value=self._value_transform(value),\n\u001b[1;32m     34\u001b[0m                                \u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reward_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1606\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1803\u001b[0m     \"\"\"\n\u001b[1;32m   1804\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1805\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1806\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m       return ParallelMapDataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4206\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4207\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   4208\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   4209\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3523\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3524\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3525\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m   3051\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3052\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3053\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   1029\u001b[0m         if x is not None)\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m     \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;31m# the function since that goes through a tf.identity in mark_as_return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m       if (op_def_registry.get(op.type) is None or\n\u001b[0;32m--> 381\u001b[0;31m           (op_is_stateful(op) and\n\u001b[0m\u001b[1;32m    382\u001b[0m            (op.type not in utils.RESOURCE_READ_OPS or\n\u001b[1;32m    383\u001b[0m             any(output.consumers() for output in op.outputs)))):\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}